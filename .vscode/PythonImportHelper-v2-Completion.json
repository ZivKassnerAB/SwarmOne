[
    {
        "label": "VisionDataset",
        "importPath": "torchvision.datasets.vision",
        "description": "torchvision.datasets.vision",
        "isExtraImport": true,
        "detail": "torchvision.datasets.vision",
        "documentation": {}
    },
    {
        "label": "VisionDataset",
        "importPath": "torchvision.datasets.vision",
        "description": "torchvision.datasets.vision",
        "isExtraImport": true,
        "detail": "torchvision.datasets.vision",
        "documentation": {}
    },
    {
        "label": "PIL",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL",
        "description": "PIL",
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "os.path",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.path",
        "description": "os.path",
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "PurePath",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "PurePath",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.utils.data",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Subset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Subset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "mask",
        "importPath": "pycocotools",
        "description": "pycocotools",
        "isExtraImport": true,
        "detail": "pycocotools",
        "documentation": {}
    },
    {
        "label": "mask",
        "importPath": "pycocotools",
        "description": "pycocotools",
        "isExtraImport": true,
        "detail": "pycocotools",
        "documentation": {}
    },
    {
        "label": "util.misc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "util.misc",
        "description": "util.misc",
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "get_local_rank",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "get_local_size",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "all_gather",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "is_main_process",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "nested_tensor_from_tensor_list",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "get_world_size",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "is_dist_avail_and_initialized",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "inverse_sigmoid",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "inverse_sigmoid",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "nested_tensor_from_tensor_list",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "nested_tensor_from_tensor_list",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "get_local_rank",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "get_local_size",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "all_gather",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "is_main_process",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "nested_tensor_from_tensor_list",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "get_world_size",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "is_dist_avail_and_initialized",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "inverse_sigmoid",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "inverse_sigmoid",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "nested_tensor_from_tensor_list",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "nested_tensor_from_tensor_list",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "datasets_internal.transforms",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datasets_internal.transforms",
        "description": "datasets_internal.transforms",
        "detail": "datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "contextlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "contextlib",
        "description": "contextlib",
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "COCOeval",
        "importPath": "pycocotools.cocoeval",
        "description": "pycocotools.cocoeval",
        "isExtraImport": true,
        "detail": "pycocotools.cocoeval",
        "documentation": {}
    },
    {
        "label": "COCOeval",
        "importPath": "pycocotools.cocoeval",
        "description": "pycocotools.cocoeval",
        "isExtraImport": true,
        "detail": "pycocotools.cocoeval",
        "documentation": {}
    },
    {
        "label": "COCO",
        "importPath": "pycocotools.coco",
        "description": "pycocotools.coco",
        "isExtraImport": true,
        "detail": "pycocotools.coco",
        "documentation": {}
    },
    {
        "label": "COCO",
        "importPath": "pycocotools.coco",
        "description": "pycocotools.coco",
        "isExtraImport": true,
        "detail": "pycocotools.coco",
        "documentation": {}
    },
    {
        "label": "pycocotools.mask",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pycocotools.mask",
        "description": "pycocotools.mask",
        "detail": "pycocotools.mask",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "rgb2id",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "rgb2id",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "util.box_ops",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "masks_to_boxes",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_xyxy_to_cxcywh",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_cxcywh_to_xyxy",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "generalized_box_iou",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_iou",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_xyxy_to_cxcywh",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_cxcywh_to_xyxy",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_cxcywh_to_xyxy",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "generalized_box_iou",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "masks_to_boxes",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_xyxy_to_cxcywh",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_cxcywh_to_xyxy",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "generalized_box_iou",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_iou",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_xyxy_to_cxcywh",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_cxcywh_to_xyxy",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_cxcywh_to_xyxy",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "generalized_box_iou",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "torch.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "Sampler",
        "importPath": "torch.utils.data.sampler",
        "description": "torch.utils.data.sampler",
        "isExtraImport": true,
        "detail": "torch.utils.data.sampler",
        "documentation": {}
    },
    {
        "label": "Sampler",
        "importPath": "torch.utils.data.sampler",
        "description": "torch.utils.data.sampler",
        "isExtraImport": true,
        "detail": "torch.utils.data.sampler",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "v2",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "v2",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "v2",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "v2",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "v2",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "xavier_uniform_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "constant_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "xavier_uniform_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "constant_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "uniform_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "xavier_uniform_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "constant_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "xavier_uniform_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "constant_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "uniform_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "torchvision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision",
        "description": "torchvision",
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "datasets",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "datasets",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "datasets",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "datasets",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "datasets",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "datasets",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "IntermediateLayerGetter",
        "importPath": "torchvision.models._utils",
        "description": "torchvision.models._utils",
        "isExtraImport": true,
        "detail": "torchvision.models._utils",
        "documentation": {}
    },
    {
        "label": "IntermediateLayerGetter",
        "importPath": "torchvision.models._utils",
        "description": "torchvision.models._utils",
        "isExtraImport": true,
        "detail": "torchvision.models._utils",
        "documentation": {}
    },
    {
        "label": "batched_nms",
        "importPath": "torchvision.ops.boxes",
        "description": "torchvision.ops.boxes",
        "isExtraImport": true,
        "detail": "torchvision.ops.boxes",
        "documentation": {}
    },
    {
        "label": "batched_nms",
        "importPath": "torchvision.ops.boxes",
        "description": "torchvision.ops.boxes",
        "isExtraImport": true,
        "detail": "torchvision.ops.boxes",
        "documentation": {}
    },
    {
        "label": "box_area",
        "importPath": "torchvision.ops.boxes",
        "description": "torchvision.ops.boxes",
        "isExtraImport": true,
        "detail": "torchvision.ops.boxes",
        "documentation": {}
    },
    {
        "label": "batched_nms",
        "importPath": "torchvision.ops.boxes",
        "description": "torchvision.ops.boxes",
        "isExtraImport": true,
        "detail": "torchvision.ops.boxes",
        "documentation": {}
    },
    {
        "label": "batched_nms",
        "importPath": "torchvision.ops.boxes",
        "description": "torchvision.ops.boxes",
        "isExtraImport": true,
        "detail": "torchvision.ops.boxes",
        "documentation": {}
    },
    {
        "label": "box_area",
        "importPath": "torchvision.ops.boxes",
        "description": "torchvision.ops.boxes",
        "isExtraImport": true,
        "detail": "torchvision.ops.boxes",
        "documentation": {}
    },
    {
        "label": "box_ops",
        "importPath": "util",
        "description": "util",
        "isExtraImport": true,
        "detail": "util",
        "documentation": {}
    },
    {
        "label": "box_ops",
        "importPath": "util",
        "description": "util",
        "isExtraImport": true,
        "detail": "util",
        "documentation": {}
    },
    {
        "label": "MSDeformAttn",
        "importPath": "models.ops.modules",
        "description": "models.ops.modules",
        "isExtraImport": true,
        "detail": "models.ops.modules",
        "documentation": {}
    },
    {
        "label": "MSDeformAttn",
        "importPath": "models.ops.modules",
        "description": "models.ops.modules",
        "isExtraImport": true,
        "detail": "models.ops.modules",
        "documentation": {}
    },
    {
        "label": "linear_sum_assignment",
        "importPath": "scipy.optimize",
        "description": "scipy.optimize",
        "isExtraImport": true,
        "detail": "scipy.optimize",
        "documentation": {}
    },
    {
        "label": "linear_sum_assignment",
        "importPath": "scipy.optimize",
        "description": "scipy.optimize",
        "isExtraImport": true,
        "detail": "scipy.optimize",
        "documentation": {}
    },
    {
        "label": "torch.utils.checkpoint",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.checkpoint",
        "description": "torch.utils.checkpoint",
        "detail": "torch.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "to_2tuple",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "to_2tuple",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "socket",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "socket",
        "description": "socket",
        "detail": "socket",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ArgumentParser",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "REMAINDER",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ArgumentParser",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "REMAINDER",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "main",
        "description": "main",
        "isExtraImport": true,
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "main",
        "description": "main",
        "isExtraImport": true,
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "build_model",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "build_model",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "build_model",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "build_model",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "build_model",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "build_model",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "datasets",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datasets",
        "description": "datasets",
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "build_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "build_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "CocoEvaluator",
        "importPath": "datasets_internal.coco_eval",
        "description": "datasets_internal.coco_eval",
        "isExtraImport": true,
        "detail": "datasets_internal.coco_eval",
        "documentation": {}
    },
    {
        "label": "CocoEvaluator",
        "importPath": "datasets_internal.coco_eval",
        "description": "datasets_internal.coco_eval",
        "isExtraImport": true,
        "detail": "datasets_internal.coco_eval",
        "documentation": {}
    },
    {
        "label": "CocoEvaluator",
        "importPath": "datasets_internal.coco_eval",
        "description": "datasets_internal.coco_eval",
        "isExtraImport": true,
        "detail": "datasets_internal.coco_eval",
        "documentation": {}
    },
    {
        "label": "PanopticEvaluator",
        "importPath": "datasets_internal.panoptic_eval",
        "description": "datasets_internal.panoptic_eval",
        "isExtraImport": true,
        "detail": "datasets_internal.panoptic_eval",
        "documentation": {}
    },
    {
        "label": "PanopticEvaluator",
        "importPath": "datasets_internal.panoptic_eval",
        "description": "datasets_internal.panoptic_eval",
        "isExtraImport": true,
        "detail": "datasets_internal.panoptic_eval",
        "documentation": {}
    },
    {
        "label": "data_prefetcher",
        "importPath": "datasets_internal.data_prefetcher",
        "description": "datasets_internal.data_prefetcher",
        "isExtraImport": true,
        "detail": "datasets_internal.data_prefetcher",
        "documentation": {}
    },
    {
        "label": "data_prefetcher",
        "importPath": "datasets_internal.data_prefetcher",
        "description": "datasets_internal.data_prefetcher",
        "isExtraImport": true,
        "detail": "datasets_internal.data_prefetcher",
        "documentation": {}
    },
    {
        "label": "build_dataset",
        "importPath": "datasets_internal",
        "description": "datasets_internal",
        "isExtraImport": true,
        "detail": "datasets_internal",
        "documentation": {}
    },
    {
        "label": "get_coco_api_from_dataset",
        "importPath": "datasets_internal",
        "description": "datasets_internal",
        "isExtraImport": true,
        "detail": "datasets_internal",
        "documentation": {}
    },
    {
        "label": "build_dataset",
        "importPath": "datasets_internal",
        "description": "datasets_internal",
        "isExtraImport": true,
        "detail": "datasets_internal",
        "documentation": {}
    },
    {
        "label": "get_coco_api_from_dataset",
        "importPath": "datasets_internal",
        "description": "datasets_internal",
        "isExtraImport": true,
        "detail": "datasets_internal",
        "documentation": {}
    },
    {
        "label": "build_dataset",
        "importPath": "datasets_internal",
        "description": "datasets_internal",
        "isExtraImport": true,
        "detail": "datasets_internal",
        "documentation": {}
    },
    {
        "label": "get_coco_api_from_dataset",
        "importPath": "datasets_internal",
        "description": "datasets_internal",
        "isExtraImport": true,
        "detail": "datasets_internal",
        "documentation": {}
    },
    {
        "label": "build_dataset",
        "importPath": "datasets_internal",
        "description": "datasets_internal",
        "isExtraImport": true,
        "detail": "datasets_internal",
        "documentation": {}
    },
    {
        "label": "get_coco_api_from_dataset",
        "importPath": "datasets_internal",
        "description": "datasets_internal",
        "isExtraImport": true,
        "detail": "datasets_internal",
        "documentation": {}
    },
    {
        "label": "lightning",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "lightning",
        "description": "lightning",
        "detail": "lightning",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "swarm_one.pytorch",
        "description": "swarm_one.pytorch",
        "isExtraImport": true,
        "detail": "swarm_one.pytorch",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "swarm_one.pytorch",
        "description": "swarm_one.pytorch",
        "isExtraImport": true,
        "detail": "swarm_one.pytorch",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "swarm_one.pytorch",
        "description": "swarm_one.pytorch",
        "isExtraImport": true,
        "detail": "swarm_one.pytorch",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "swarm_one.pytorch",
        "description": "swarm_one.pytorch",
        "isExtraImport": true,
        "detail": "swarm_one.pytorch",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "swarm_one.pytorch",
        "description": "swarm_one.pytorch",
        "isExtraImport": true,
        "detail": "swarm_one.pytorch",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "swarm_one.pytorch",
        "description": "swarm_one.pytorch",
        "isExtraImport": true,
        "detail": "swarm_one.pytorch",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "swarm_one.pytorch",
        "description": "swarm_one.pytorch",
        "isExtraImport": true,
        "detail": "swarm_one.pytorch",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "swarm_one.pytorch",
        "description": "swarm_one.pytorch",
        "isExtraImport": true,
        "detail": "swarm_one.pytorch",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "swarm_one.pytorch",
        "description": "swarm_one.pytorch",
        "isExtraImport": true,
        "detail": "swarm_one.pytorch",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "swarm_one.pytorch",
        "description": "swarm_one.pytorch",
        "isExtraImport": true,
        "detail": "swarm_one.pytorch",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "swarm_one.pytorch",
        "description": "swarm_one.pytorch",
        "isExtraImport": true,
        "detail": "swarm_one.pytorch",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "swarm_one.pytorch",
        "description": "swarm_one.pytorch",
        "isExtraImport": true,
        "detail": "swarm_one.pytorch",
        "documentation": {}
    },
    {
        "label": "log_job_metrics",
        "importPath": "logging_swarmone_clearml",
        "description": "logging_swarmone_clearml",
        "isExtraImport": true,
        "detail": "logging_swarmone_clearml",
        "documentation": {}
    },
    {
        "label": "ModelCheckpoint",
        "importPath": "pytorch_lightning.callbacks",
        "description": "pytorch_lightning.callbacks",
        "isExtraImport": true,
        "detail": "pytorch_lightning.callbacks",
        "documentation": {}
    },
    {
        "label": "ModelCheckpoint",
        "importPath": "pytorch_lightning.callbacks",
        "description": "pytorch_lightning.callbacks",
        "isExtraImport": true,
        "detail": "pytorch_lightning.callbacks",
        "documentation": {}
    },
    {
        "label": "pytorch_lightning",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytorch_lightning",
        "description": "pytorch_lightning",
        "detail": "pytorch_lightning",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "pytorch_lightning",
        "description": "pytorch_lightning",
        "isExtraImport": true,
        "detail": "pytorch_lightning",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "pytorch_lightning",
        "description": "pytorch_lightning",
        "isExtraImport": true,
        "detail": "pytorch_lightning",
        "documentation": {}
    },
    {
        "label": "STL10",
        "importPath": "torchvision.datasets",
        "description": "torchvision.datasets",
        "isExtraImport": true,
        "detail": "torchvision.datasets",
        "documentation": {}
    },
    {
        "label": "STL10",
        "importPath": "torchvision.datasets",
        "description": "torchvision.datasets",
        "isExtraImport": true,
        "detail": "torchvision.datasets",
        "documentation": {}
    },
    {
        "label": "torchmetrics",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchmetrics",
        "description": "torchmetrics",
        "detail": "torchmetrics",
        "documentation": {}
    },
    {
        "label": "mobilenet_v2",
        "importPath": "torchvision.models",
        "description": "torchvision.models",
        "isExtraImport": true,
        "detail": "torchvision.models",
        "documentation": {}
    },
    {
        "label": "mobilenet_v2",
        "importPath": "torchvision.models",
        "description": "torchvision.models",
        "isExtraImport": true,
        "detail": "torchvision.models",
        "documentation": {}
    },
    {
        "label": "torch.optim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.optim",
        "description": "torch.optim",
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "wandb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wandb",
        "description": "wandb",
        "detail": "wandb",
        "documentation": {}
    },
    {
        "label": "MeanAveragePrecision",
        "importPath": "torchmetrics.detection",
        "description": "torchmetrics.detection",
        "isExtraImport": true,
        "detail": "torchmetrics.detection",
        "documentation": {}
    },
    {
        "label": "MeanAveragePrecision",
        "importPath": "torchmetrics.detection",
        "description": "torchmetrics.detection",
        "isExtraImport": true,
        "detail": "torchmetrics.detection",
        "documentation": {}
    },
    {
        "label": "MeanAveragePrecision",
        "importPath": "torchmetrics.detection",
        "description": "torchmetrics.detection",
        "isExtraImport": true,
        "detail": "torchmetrics.detection",
        "documentation": {}
    },
    {
        "label": "Task",
        "importPath": "clearml",
        "description": "clearml",
        "isExtraImport": true,
        "detail": "clearml",
        "documentation": {}
    },
    {
        "label": "CocoDetection",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.torchvision_datasets.coco",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.torchvision_datasets.coco",
        "peekOfCode": "class CocoDetection(VisionDataset):\n    \"\"\"`MS Coco Detection <http://mscoco.org/dataset/#detections-challenge2016>`_ Dataset.\n    Args:\n        root (string): Root directory where images are downloaded to.\n        annFile (string): Path to json annotation file.\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.ToTensor``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        transforms (callable, optional): A function/transform that takes input sample and its target as entry",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.torchvision_datasets.coco",
        "documentation": {}
    },
    {
        "label": "CocoDetection",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "peekOfCode": "class CocoDetection(TvCocoDetection):\n    def __init__(self, img_folder, ann_file, transforms, return_masks, cache_mode=False, local_rank=0, local_size=1):\n        super(CocoDetection, self).__init__(img_folder, ann_file,\n                                            cache_mode=cache_mode, local_rank=local_rank, local_size=local_size)\n        self.prepare = ConvertCocoPolysToMask(return_masks)\n        self._transforms = lambda r: transforms(r[0], r[1])\n        self.swarm_one_transform_name = \"_transforms\"\n    def __getitem__(self, idx):\n        img, target = super(CocoDetection, self).__getitem__(idx)\n        image_id = self.ids[idx]",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "documentation": {}
    },
    {
        "label": "ConvertCocoPolysToMask",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "peekOfCode": "class ConvertCocoPolysToMask(object):\n    def __init__(self, return_masks=False):\n        self.return_masks = return_masks\n    def __call__(self, image, target):\n        w, h = image.size\n        image_id = target[\"image_id\"]\n        image_id = torch.tensor([image_id])\n        anno = target[\"annotations\"]\n        anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n        boxes = [obj[\"bbox\"] for obj in anno]",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "documentation": {}
    },
    {
        "label": "convert_coco_poly_to_mask",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "peekOfCode": "def convert_coco_poly_to_mask(segmentations, height, width):\n    masks = []\n    for polygons in segmentations:\n        rles = coco_mask.frPyObjects(polygons, height, width)\n        mask = coco_mask.decode(rles)\n        if len(mask.shape) < 3:\n            mask = mask[..., None]\n        mask = torch.as_tensor(mask, dtype=torch.uint8)\n        mask = mask.any(dim=2)\n        masks.append(mask)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "documentation": {}
    },
    {
        "label": "make_coco_transforms",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "peekOfCode": "def make_coco_transforms(image_set, bigger):\n    normalize = T.Compose([\n        T.ToTensor(),\n        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    if 'train' in image_set:\n        scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n    if 'val' in image_set or 'test' in image_set:\n        scales = [800]\n    max_size = 1333",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "documentation": {}
    },
    {
        "label": "build",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "peekOfCode": "def build(image_set, args):\n    root = Path(args.coco_path)\n    assert root.exists(), f'provided COCO path {root} does not exist'\n    mode = 'instances'\n    PATHS = {\n        \"train\": (\"/home/user/workspace/swarmone/swarm_poc/data/train_subset\", '/home/user/workspace/swarmone/swarm_poc/data/train_subset_.json'),\n        \"val\": (\"/home/user/workspace/swarmone/swarm_poc/data/val_subset\", '/home/user/workspace/swarmone/swarm_poc/data/val_subset_.json')\n    }\n    img_folder, ann_file = PATHS[image_set]\n    dataset = CocoDetection(img_folder, ann_file, transforms=make_coco_transforms(image_set, args.bigger), return_masks=args.masks,",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "documentation": {}
    },
    {
        "label": "CocoEvaluator",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "peekOfCode": "class CocoEvaluator(object):\n    def __init__(self, coco_gt, iou_types):\n        assert isinstance(iou_types, (list, tuple))\n        coco_gt = copy.deepcopy(coco_gt)\n        self.coco_gt = coco_gt\n        self.iou_types = iou_types\n        self.coco_eval = {}\n        for iou_type in iou_types:\n            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n        self.img_ids = []",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "documentation": {}
    },
    {
        "label": "convert_to_xywh",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "peekOfCode": "def convert_to_xywh(boxes):\n    xmin, ymin, xmax, ymax = boxes.unbind(1)\n    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\ndef merge(img_ids, eval_imgs):\n    all_img_ids = all_gather(img_ids)\n    all_eval_imgs = all_gather(eval_imgs)\n    merged_img_ids = []\n    for p in all_img_ids:\n        merged_img_ids.extend(p)\n    merged_eval_imgs = []",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "documentation": {}
    },
    {
        "label": "merge",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "peekOfCode": "def merge(img_ids, eval_imgs):\n    all_img_ids = all_gather(img_ids)\n    all_eval_imgs = all_gather(eval_imgs)\n    merged_img_ids = []\n    for p in all_img_ids:\n        merged_img_ids.extend(p)\n    merged_eval_imgs = []\n    for p in all_eval_imgs:\n        merged_eval_imgs.append(p)\n    merged_img_ids = np.array(merged_img_ids)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "documentation": {}
    },
    {
        "label": "create_common_coco_eval",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "peekOfCode": "def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n    img_ids = list(img_ids)\n    eval_imgs = list(eval_imgs.flatten())\n    coco_eval.evalImgs = eval_imgs\n    coco_eval.params.imgIds = img_ids\n    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n#################################################################\n# From pycocotools, just removed the prints and fixed\n# a Python3 bug about unicode not defined",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "peekOfCode": "def evaluate(self):\n    '''\n    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n    :return: None\n    '''\n    # tic = time.time()\n    # print('Running per image evaluation...')\n    p = self.params\n    # add backward compatibility if useSegm is specified in params\n    if p.useSegm is not None:",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "documentation": {}
    },
    {
        "label": "CocoPanoptic",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_panoptic",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_panoptic",
        "peekOfCode": "class CocoPanoptic:\n    def __init__(self, img_folder, ann_folder, ann_file, transforms=None, return_masks=True):\n        with open(ann_file, 'r') as f:\n            self.coco = json.load(f)\n        # sort 'images' field so that they are aligned with 'annotations'\n        # i.e., in alphabetical order\n        self.coco['images'] = sorted(self.coco['images'], key=lambda x: x['id'])\n        # sanity check\n        if \"annotations\" in self.coco:\n            for img, ann in zip(self.coco['images'], self.coco['annotations']):",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_panoptic",
        "documentation": {}
    },
    {
        "label": "build",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_panoptic",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_panoptic",
        "peekOfCode": "def build(image_set, args):\n    img_folder_root = Path(args.coco_path)\n    ann_folder_root = Path(args.coco_panoptic_path)\n    assert img_folder_root.exists(), f'provided COCO path {img_folder_root} does not exist'\n    assert ann_folder_root.exists(), f'provided COCO path {ann_folder_root} does not exist'\n    mode = 'panoptic'\n    PATHS = {\n        \"train\": (\"train2017\", Path(\"annotations\") / f'{mode}_train2017.json'),\n        \"val\": (\"val2017\", Path(\"annotations\") / f'{mode}_val2017.json'),\n    }",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_panoptic",
        "documentation": {}
    },
    {
        "label": "data_prefetcher",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.data_prefetcher",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.data_prefetcher",
        "peekOfCode": "class data_prefetcher():\n    def __init__(self, loader, device, prefetch=True):\n        self.loader = iter(loader)\n        self.prefetch = prefetch\n        self.device = device\n        if prefetch:\n            self.stream = torch.cuda.Stream()\n            self.preload()\n    def preload(self):\n        try:",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.data_prefetcher",
        "documentation": {}
    },
    {
        "label": "to_cuda",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.data_prefetcher",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.data_prefetcher",
        "peekOfCode": "def to_cuda(samples, targets, device):\n    samples = samples.to(device, non_blocking=True)\n    targets = [{k: v.to(device, non_blocking=True) for k, v in t.items()} for t in targets]\n    return samples, targets\nclass data_prefetcher():\n    def __init__(self, loader, device, prefetch=True):\n        self.loader = iter(loader)\n        self.prefetch = prefetch\n        self.device = device\n        if prefetch:",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.data_prefetcher",
        "documentation": {}
    },
    {
        "label": "PanopticEvaluator",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.panoptic_eval",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.panoptic_eval",
        "peekOfCode": "class PanopticEvaluator(object):\n    def __init__(self, ann_file, ann_folder, output_dir=\"panoptic_eval\"):\n        self.gt_json = ann_file\n        self.gt_folder = ann_folder\n        if utils.is_main_process():\n            if not os.path.exists(output_dir):\n                os.mkdir(output_dir)\n        self.output_dir = output_dir\n        self.predictions = []\n    def update(self, predictions):",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.panoptic_eval",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.samplers",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.samplers",
        "peekOfCode": "class DistributedSampler(Sampler):\n    \"\"\"Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.samplers",
        "documentation": {}
    },
    {
        "label": "NodeDistributedSampler",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.samplers",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.samplers",
        "peekOfCode": "class NodeDistributedSampler(Sampler):\n    \"\"\"Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.samplers",
        "documentation": {}
    },
    {
        "label": "RandomCrop",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class RandomCrop(object):\n    def __init__(self, size):\n        self.size = size\n    def __call__(self, img, target):\n        region = T.RandomCrop.get_params(img, self.size)\n        return crop(img, target, region)\nclass RandomSizeCrop(object):\n    def __init__(self, min_size: int, max_size: int):\n        self.min_size = min_size\n        self.max_size = max_size",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "RandomSizeCrop",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class RandomSizeCrop(object):\n    def __init__(self, min_size: int, max_size: int):\n        self.min_size = min_size\n        self.max_size = max_size\n    def __call__(self, img: PIL.Image.Image, target: dict):\n        w = random.randint(self.min_size, min(img.width, self.max_size))\n        h = random.randint(self.min_size, min(img.height, self.max_size))\n        region = T.RandomCrop.get_params(img, [h, w])\n        return crop(img, target, region)\nclass CenterCrop(object):",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "CenterCrop",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class CenterCrop(object):\n    def __init__(self, size):\n        self.size = size\n    def __call__(self, img, target):\n        image_width, image_height = img.size\n        crop_height, crop_width = self.size\n        crop_top = int(round((image_height - crop_height) / 2.))\n        crop_left = int(round((image_width - crop_width) / 2.))\n        return crop(img, target, (crop_top, crop_left, crop_height, crop_width))\nclass RandomHorizontalFlip(object):",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "RandomHorizontalFlip",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class RandomHorizontalFlip(object):\n    def __init__(self, p=0.5):\n        self.p = p\n    def __call__(self, img, target):\n        if random.random() < self.p:\n            return hflip(img, target)\n        return img, target\nclass RandomResize(object):\n    def __init__(self, sizes, max_size=None):\n        assert isinstance(sizes, (list, tuple))",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "RandomResize",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class RandomResize(object):\n    def __init__(self, sizes, max_size=None):\n        assert isinstance(sizes, (list, tuple))\n        self.sizes = sizes\n        self.max_size = max_size\n    def __call__(self, img, target=None):\n        size = random.choice(self.sizes)\n        return resize(img, target, size, self.max_size)\nclass RandomPad(object):\n    def __init__(self, max_pad):",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "RandomPad",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class RandomPad(object):\n    def __init__(self, max_pad):\n        self.max_pad = max_pad\n    def __call__(self, img, target):\n        pad_x = random.randint(0, self.max_pad)\n        pad_y = random.randint(0, self.max_pad)\n        return pad(img, target, (pad_x, pad_y))\nclass RandomSelect(object):\n    \"\"\"\n    Randomly selects between transforms1 and transforms2,",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "RandomSelect",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class RandomSelect(object):\n    \"\"\"\n    Randomly selects between transforms1 and transforms2,\n    with probability p for transforms1 and (1 - p) for transforms2\n    \"\"\"\n    def __init__(self, transforms1, transforms2, p=0.5):\n        self.transforms1 = transforms1\n        self.transforms2 = transforms2\n        self.p = p\n    def __call__(self, img, target):",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "ToTensor",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class ToTensor(object):\n    def __call__(self, img, target):\n        return v2.ToTensor()(img), target\nclass RandomErasing(object):\n    def __init__(self, *args, **kwargs):\n        self.eraser = T.RandomErasing(*args, **kwargs)\n    def __call__(self, img, target):\n        return self.eraser(img), target\nclass Normalize(object):\n    def __init__(self, mean, std):",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "RandomErasing",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class RandomErasing(object):\n    def __init__(self, *args, **kwargs):\n        self.eraser = T.RandomErasing(*args, **kwargs)\n    def __call__(self, img, target):\n        return self.eraser(img), target\nclass Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n    def __call__(self, image, target=None):",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "Normalize",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n    def __call__(self, image, target=None):\n        image = F.normalize(image, mean=self.mean, std=self.std)\n        if target is None:\n            return image, None\n        target = target.copy()\n        h, w = image.shape[-2:]",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \"(\"\n        for t in self.transforms:",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "crop",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "def crop(image, target, region):\n    cropped_image = F.crop(image, *region)\n    target = target.copy()\n    i, j, h, w = region\n    # should we do something wrt the original size?\n    target[\"size\"] = torch.tensor([h, w])\n    fields = [\"labels\", \"area\", \"iscrowd\"]\n    if \"boxes\" in target:\n        boxes = target[\"boxes\"]\n        max_size = torch.as_tensor([w, h], dtype=torch.float32)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "hflip",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "def hflip(image, target):\n    flipped_image = F.hflip(image)\n    w, h = image.size\n    target = target.copy()\n    if \"boxes\" in target:\n        boxes = target[\"boxes\"]\n        boxes = boxes[:, [2, 1, 0, 3]] * torch.as_tensor([-1, 1, -1, 1]) + torch.as_tensor([w, 0, w, 0])\n        target[\"boxes\"] = boxes\n    if \"masks\" in target:\n        target['masks'] = target['masks'].flip(-1)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "resize",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "def resize(image, target, size, max_size=None):\n    # size can be min_size (scalar) or (w, h) tuple\n    def get_size_with_aspect_ratio(image_size, size, max_size=None):\n        w, h = image_size\n        if max_size is not None:\n            min_original_size = float(min((w, h)))\n            max_original_size = float(max((w, h)))\n            if max_original_size / min_original_size * size > max_size:\n                size = int(round(max_size * min_original_size / max_original_size))\n        if (w <= h and w == size) or (h <= w and h == size):",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "pad",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "def pad(image, target, padding):\n    # assumes that we only pad on the bottom right corners\n    padded_image = F.pad(image, (0, 0, padding[0], padding[1]))\n    if target is None:\n        return padded_image, None\n    target = target.copy()\n    # should we do something wrt the original size?\n    target[\"size\"] = torch.tensor(padded_image[::-1])\n    if \"masks\" in target:\n        target['masks'] = torch.nn.functional.pad(target['masks'], (0, padding[0], 0, padding[1]))",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "ms_deform_attn_core_pytorch",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.ops.functions.ms_deform_attn_func",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.ops.functions.ms_deform_attn_func",
        "peekOfCode": "def ms_deform_attn_core_pytorch(value, value_spatial_shapes, sampling_locations, attention_weights):\n    # for debug and test only,\n    # need to use cuda version instead\n    N_, S_, M_, D_ = value.shape\n    _, Lq_, M_, L_, P_, _ = sampling_locations.shape\n    value_list = value.split([H_ * W_ for H_, W_ in value_spatial_shapes], dim=1)\n    sampling_grids = 2 * sampling_locations - 1\n    sampling_value_list = []\n    for lid_, (H_, W_) in enumerate(value_spatial_shapes):\n        # N_, H_*W_, M_, D_ -> N_, H_*W_, M_*D_ -> N_, M_*D_, H_*W_ -> N_*M_, D_, H_, W_",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.ops.functions.ms_deform_attn_func",
        "documentation": {}
    },
    {
        "label": "MSDeformAttn",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.ops.modules.ms_deform_attn",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.ops.modules.ms_deform_attn",
        "peekOfCode": "class MSDeformAttn(nn.Module):\n    def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):\n        \"\"\"\n        Multi-Scale Deformable Attention Module\n        :param d_model      hidden dimension\n        :param n_levels     number of feature levels\n        :param n_heads      number of attention heads\n        :param n_points     number of sampling points per attention head per feature level\n        \"\"\"\n        super().__init__()",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.ops.modules.ms_deform_attn",
        "documentation": {}
    },
    {
        "label": "Matcher",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "peekOfCode": "class Matcher(object):\n    \"\"\"\n    This class assigns to each predicted \"element\" (e.g., a box) a ground-truth\n    element. Each predicted element will have exactly zero or one matches; each\n    ground-truth element may be matched to zero or more predicted elements.\n    The matching is determined by the MxN match_quality_matrix, that characterizes\n    how well each (ground-truth, prediction)-pair match each other. For example,\n    if the elements are boxes, this matrix may contain box intersection-over-union\n    overlap values.\n    The matcher returns (a) a vector of length N containing the index of the",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "documentation": {}
    },
    {
        "label": "Stage2Assigner",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "peekOfCode": "class Stage2Assigner(nn.Module):\n    def __init__(self, num_queries, max_k=4):\n        super().__init__()\n        self.positive_fraction = 0.25\n        self.bg_label = 400  # number > 91 to filter out later\n        self.batch_size_per_image = num_queries\n        self.proposal_matcher = Matcher(thresholds=[0.6], labels=[0, 1], allow_low_quality_matches=True)\n        self.k = max_k\n    def _sample_proposals(\n        self, matched_idxs: torch.Tensor, matched_labels: torch.Tensor, gt_classes: torch.Tensor",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "documentation": {}
    },
    {
        "label": "Stage1Assigner",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "peekOfCode": "class Stage1Assigner(nn.Module):\n    def __init__(self, t_low=0.3, t_high=0.7, max_k=4):\n        super().__init__()\n        self.positive_fraction = 0.5\n        self.batch_size_per_image = 256\n        self.k = max_k\n        self.t_low = t_low\n        self.t_high = t_high\n        self.anchor_matcher = Matcher(thresholds=[t_low, t_high], labels=[0, -1, 1], allow_low_quality_matches=True)\n    def _subsample_labels(self, label):",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "documentation": {}
    },
    {
        "label": "nonzero_tuple",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "peekOfCode": "def nonzero_tuple(x):\n    \"\"\"\n    A 'as_tuple=True' version of torch.nonzero to support torchscript.\n    because of https://github.com/pytorch/pytorch/issues/38718\n    \"\"\"\n    if torch.jit.is_scripting():\n        if x.dim() == 0:\n            return x.unsqueeze(0).nonzero().unbind(1)\n        return x.nonzero().unbind(1)\n    else:",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "documentation": {}
    },
    {
        "label": "subsample_labels",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "peekOfCode": "def subsample_labels(\n    labels: torch.Tensor, num_samples: int, positive_fraction: float, bg_label: int\n):\n    \"\"\"\n    Return `num_samples` (or fewer, if not enough found)\n    random samples from `labels` which is a mixture of positives & negatives.\n    It will try to return as many positives as possible without\n    exceeding `positive_fraction * num_samples`, and then try to\n    fill the remaining slots with negatives.\n    Args:",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "documentation": {}
    },
    {
        "label": "sample_topk_per_gt",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "peekOfCode": "def sample_topk_per_gt(pr_inds, gt_inds, iou, k):\n    if len(gt_inds) == 0:\n        return pr_inds, gt_inds\n    # find topk matches for each gt\n    gt_inds2, counts = gt_inds.unique(return_counts=True)\n    scores, pr_inds2 = iou[gt_inds2].topk(k, dim=1)\n    gt_inds2 = gt_inds2[:,None].repeat(1, k)\n    # filter to as many matches that gt has\n    pr_inds3 = torch.cat([pr[:c] for c, pr in zip(counts, pr_inds2)]).to(pr_inds2.device)\n    gt_inds3 = torch.cat([gt[:c] for c, gt in zip(counts, gt_inds2)]).to(pr_inds2.device)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "documentation": {}
    },
    {
        "label": "FrozenBatchNorm2d",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "peekOfCode": "class FrozenBatchNorm2d(torch.nn.Module):\n    \"\"\"\n    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n    without which any other models than torchvision.models.resnet[18,34,50,101]\n    produce nans.\n    \"\"\"\n    def __init__(self, n, eps=1e-5):\n        super(FrozenBatchNorm2d, self).__init__()\n        self.register_buffer(\"weight\", torch.ones(n))",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "documentation": {}
    },
    {
        "label": "BackboneBase",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "peekOfCode": "class BackboneBase(nn.Module):\n    def __init__(self, backbone: nn.Module, train_backbone: bool, return_interm_layers: bool):\n        super().__init__()\n        for name, parameter in backbone.named_parameters():\n            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n                parameter.requires_grad_(False)\n        if return_interm_layers:\n            # return_layers = {\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"}\n            return_layers = {\"layer2\": \"0\", \"layer3\": \"1\", \"layer4\": \"2\"}\n            self.strides = [8, 16, 32]",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "documentation": {}
    },
    {
        "label": "Backbone",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "peekOfCode": "class Backbone(BackboneBase):\n    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n    def __init__(self, name: str,\n                 train_backbone: bool,\n                 return_interm_layers: bool,\n                 dilation: bool):\n        norm_layer = FrozenBatchNorm2d\n        backbone = getattr(torchvision.models, name)(\n            replace_stride_with_dilation=[False, False, dilation],\n            pretrained=is_main_process(), norm_layer=norm_layer)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "documentation": {}
    },
    {
        "label": "SwinBackbone",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "peekOfCode": "class SwinBackbone(nn.Module):\n    def __init__(self):\n        # we skip R50 FrozenBatchNorm2d, dilation, train l{2,3,4} only\n        super().__init__()\n        self.body = get_swinl()\n        self.features = ['res3', 'res4', 'res5']\n        self.strides = [8, 16, 32]\n        self.num_channels = [384, 768, 1536]\n    def forward(self, tensor_list: NestedTensor):\n        xs = self.body(tensor_list.tensors)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "documentation": {}
    },
    {
        "label": "Joiner",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "peekOfCode": "class Joiner(nn.Sequential):\n    def __init__(self, backbone, position_embedding):\n        super().__init__(backbone, position_embedding)\n        self.strides = backbone.strides\n        self.num_channels = backbone.num_channels\n    def forward(self, tensor_list: NestedTensor):\n        xs = self[0](tensor_list)\n        out: List[NestedTensor] = []\n        pos = []\n        for name, x in sorted(xs.items()):",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "documentation": {}
    },
    {
        "label": "build_backbone",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "peekOfCode": "def build_backbone(args):\n    position_embedding = build_position_encoding(args)\n    train_backbone = args.lr_backbone > 0\n    return_interm_layers = args.masks or (args.num_feature_levels > 1)\n    if 'swin' in args.backbone:\n        backbone = SwinBackbone()\n    else:\n        backbone = Backbone(args.backbone, train_backbone, return_interm_layers, args.dilation)\n    model = Joiner(backbone, position_embedding)\n    return model",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "documentation": {}
    },
    {
        "label": "DeformableDETR",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "peekOfCode": "class DeformableDETR(nn.Module):\n    \"\"\" This is the Deformable DETR module that performs object detection \"\"\"\n    def __init__(self, backbone, transformer, num_classes, num_queries, num_feature_levels,\n                 aux_loss=True, with_box_refine=False, two_stage=False):\n        \"\"\" Initializes the model.\n        Parameters:\n            backbone: torch module of the backbone to be used. See backbone.py\n            transformer: torch module of the transformer architecture. See transformer.py\n            num_classes: number of object classes\n            num_queries: number of object queries, ie detection slot. This is the maximal number of objects",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "documentation": {}
    },
    {
        "label": "SetCriterion",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "peekOfCode": "class SetCriterion(nn.Module):\n    \"\"\" This class computes the loss for DETR.\n    The process happens in two steps:\n        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n    \"\"\"\n    def __init__(self, num_classes, matcher, weight_dict, losses, focal_alpha=0.25,\n                 num_queries=300, assign_first_stage=False, assign_second_stage=False):\n        \"\"\" Create the criterion.\n        Parameters:",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "documentation": {}
    },
    {
        "label": "PostProcess",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "peekOfCode": "class PostProcess(nn.Module):\n    \"\"\" This module converts the model's output into the format expected by the coco api\"\"\"\n    @torch.no_grad()\n    def forward(self, outputs, target_sizes):\n        \"\"\" Perform the computation\n        Parameters:\n            outputs: raw outputs of the model\n            target_sizes: tensor of dimension [batch_size x 2] containing the size of each images of the batch\n                          For evaluation, this must be the original image size (before any data augmentation)\n                          For visualization, this should be the image size after data augment, but before padding",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "documentation": {}
    },
    {
        "label": "NMSPostProcess",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "peekOfCode": "class NMSPostProcess(nn.Module):\n    \"\"\" This module converts the model's output into the format expected by the coco api\"\"\"\n    @torch.no_grad()\n    def forward(self, outputs, target_sizes):\n        \"\"\" Perform the computation\n        Parameters:\n            outputs: raw outputs of the model\n            target_sizes: tensor of dimension [batch_size x 2] containing the size of each images of the batch\n                          For evaluation, this must be the original image size (before any data augmentation)\n                          For visualization, this should be the image size after data augment, but before padding",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "peekOfCode": "class MLP(nn.Module):\n    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n        super().__init__()\n        self.num_layers = num_layers\n        h = [hidden_dim] * (num_layers - 1)\n        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n    def forward(self, x):\n        for i, layer in enumerate(self.layers):\n            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "documentation": {}
    },
    {
        "label": "build",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "peekOfCode": "def build(args):\n    num_classes = 20 if args.dataset_file != 'coco' else 91\n    if args.dataset_file == \"coco_panoptic\":\n        num_classes = 250\n    device = torch.device(args.device)\n    backbone = build_backbone(args)\n    transformer = build_deforamble_transformer(args)\n    model = DeformableDETR(\n        backbone,\n        transformer,",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "documentation": {}
    },
    {
        "label": "DeformableTransformer",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "peekOfCode": "class DeformableTransformer(nn.Module):\n    def __init__(self, d_model=256, nhead=8,\n                 num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=1024, dropout=0.1,\n                 activation=\"relu\", return_intermediate_dec=False,\n                 num_feature_levels=4, dec_n_points=4,  enc_n_points=4,\n                 two_stage=False, two_stage_num_proposals=300,\n                 assign_first_stage=False):\n        super().__init__()\n        self.d_model = d_model\n        self.nhead = nhead",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "documentation": {}
    },
    {
        "label": "DeformableTransformerEncoderLayer",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "peekOfCode": "class DeformableTransformerEncoderLayer(nn.Module):\n    def __init__(self,\n                 d_model=256, d_ffn=1024,\n                 dropout=0.1, activation=\"relu\",\n                 n_levels=4, n_heads=8, n_points=4):\n        super().__init__()\n        # self attention\n        self.self_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)\n        self.dropout1 = nn.Dropout(dropout)\n        self.norm1 = nn.LayerNorm(d_model)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "documentation": {}
    },
    {
        "label": "DeformableTransformerEncoder",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "peekOfCode": "class DeformableTransformerEncoder(nn.Module):\n    def __init__(self, encoder_layer, num_layers):\n        super().__init__()\n        self.layers = _get_clones(encoder_layer, num_layers)\n        self.num_layers = num_layers\n    @staticmethod\n    def get_reference_points(spatial_shapes, valid_ratios, device):\n        reference_points_list = []\n        for lvl, (H_, W_) in enumerate(spatial_shapes):\n            ref_y, ref_x = torch.meshgrid(torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32, device=device),",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "documentation": {}
    },
    {
        "label": "DeformableTransformerDecoderLayer",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "peekOfCode": "class DeformableTransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model=256, d_ffn=1024,\n                 dropout=0.1, activation=\"relu\",\n                 n_levels=4, n_heads=8, n_points=4):\n        super().__init__()\n        # cross attention\n        self.cross_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)\n        self.dropout1 = nn.Dropout(dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        # self attention",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "documentation": {}
    },
    {
        "label": "DeformableTransformerDecoder",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "peekOfCode": "class DeformableTransformerDecoder(nn.Module):\n    def __init__(self, decoder_layer, num_layers, return_intermediate=False):\n        super().__init__()\n        self.layers = _get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.return_intermediate = return_intermediate\n        # hack implementation for iterative bounding box refinement and two-stage Deformable DETR\n        self.bbox_embed = None\n        self.class_embed = None\n    def forward(self, tgt, reference_points, src, src_spatial_shapes, src_level_start_index, src_valid_ratios,",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "documentation": {}
    },
    {
        "label": "build_deforamble_transformer",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "peekOfCode": "def build_deforamble_transformer(args):\n    return DeformableTransformer(\n        d_model=args.hidden_dim,\n        nhead=args.nheads,\n        num_encoder_layers=args.enc_layers,\n        num_decoder_layers=args.dec_layers,\n        dim_feedforward=args.dim_feedforward,\n        dropout=args.dropout,\n        activation=\"relu\",\n        return_intermediate_dec=True,",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "documentation": {}
    },
    {
        "label": "HungarianMatcher",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.matcher",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.matcher",
        "peekOfCode": "class HungarianMatcher(nn.Module):\n    \"\"\"This class computes an assignment between the targets and the predictions of the network\n    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n    while the others are un-matched (and thus treated as non-objects).\n    \"\"\"\n    def __init__(self,\n                 cost_class: float = 1,\n                 cost_bbox: float = 1,\n                 cost_giou: float = 1):",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.matcher",
        "documentation": {}
    },
    {
        "label": "build_matcher",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.matcher",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.matcher",
        "peekOfCode": "def build_matcher(args):\n    return HungarianMatcher(cost_class=args.set_cost_class,\n                            cost_bbox=args.set_cost_bbox,\n                            cost_giou=args.set_cost_giou)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.matcher",
        "documentation": {}
    },
    {
        "label": "PositionEmbeddingSine",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.position_encoding",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.position_encoding",
        "peekOfCode": "class PositionEmbeddingSine(nn.Module):\n    \"\"\"\n    This is a more standard version of the position embedding, very similar to the one\n    used by the Attention is all you need paper, generalized to work on images.\n    \"\"\"\n    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n        super().__init__()\n        self.num_pos_feats = num_pos_feats\n        self.temperature = temperature\n        self.normalize = normalize",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.position_encoding",
        "documentation": {}
    },
    {
        "label": "PositionEmbeddingLearned",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.position_encoding",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.position_encoding",
        "peekOfCode": "class PositionEmbeddingLearned(nn.Module):\n    \"\"\"\n    Absolute pos embedding, learned.\n    \"\"\"\n    def __init__(self, num_pos_feats=256):\n        super().__init__()\n        self.row_embed = nn.Embedding(50, num_pos_feats)\n        self.col_embed = nn.Embedding(50, num_pos_feats)\n        self.reset_parameters()\n    def reset_parameters(self):",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.position_encoding",
        "documentation": {}
    },
    {
        "label": "build_position_encoding",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.position_encoding",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.position_encoding",
        "peekOfCode": "def build_position_encoding(args):\n    N_steps = args.hidden_dim // 2\n    if args.position_embedding in ('v2', 'sine'):\n        # TODO find a better way of exposing other arguments\n        position_embedding = PositionEmbeddingSine(N_steps, normalize=True)\n    elif args.position_embedding in ('v3', 'learned'):\n        position_embedding = PositionEmbeddingLearned(N_steps)\n    else:\n        raise ValueError(f\"not supported {args.position_embedding}\")\n    return position_embedding",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.position_encoding",
        "documentation": {}
    },
    {
        "label": "DETRsegm",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "peekOfCode": "class DETRsegm(nn.Module):\n    def __init__(self, detr, freeze_detr=False):\n        super().__init__()\n        self.detr = detr\n        if freeze_detr:\n            for p in self.parameters():\n                p.requires_grad_(False)\n        hidden_dim, nheads = detr.transformer.d_model, detr.transformer.nhead\n        self.bbox_attention = MHAttentionMap(hidden_dim, hidden_dim, nheads, dropout=0)\n        self.mask_head = MaskHeadSmallConv(hidden_dim + nheads, [1024, 512, 256], hidden_dim)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "documentation": {}
    },
    {
        "label": "MaskHeadSmallConv",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "peekOfCode": "class MaskHeadSmallConv(nn.Module):\n    \"\"\"\n    Simple convolutional head, using group norm.\n    Upsampling is done using a FPN approach\n    \"\"\"\n    def __init__(self, dim, fpn_dims, context_dim):\n        super().__init__()\n        inter_dims = [dim, context_dim // 2, context_dim // 4, context_dim // 8, context_dim // 16, context_dim // 64]\n        self.lay1 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n        self.gn1 = torch.nn.GroupNorm(8, dim)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "documentation": {}
    },
    {
        "label": "MHAttentionMap",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "peekOfCode": "class MHAttentionMap(nn.Module):\n    \"\"\"This is a 2D attention module, which only returns the attention softmax (no multiplication by value)\"\"\"\n    def __init__(self, query_dim, hidden_dim, num_heads, dropout=0, bias=True):\n        super().__init__()\n        self.num_heads = num_heads\n        self.hidden_dim = hidden_dim\n        self.dropout = nn.Dropout(dropout)\n        self.q_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n        self.k_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n        nn.init.zeros_(self.k_linear.bias)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "documentation": {}
    },
    {
        "label": "PostProcessSegm",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "peekOfCode": "class PostProcessSegm(nn.Module):\n    def __init__(self, threshold=0.5):\n        super().__init__()\n        self.threshold = threshold\n    @torch.no_grad()\n    def forward(self, results, outputs, orig_target_sizes, max_target_sizes):\n        assert len(orig_target_sizes) == len(max_target_sizes)\n        max_h, max_w = max_target_sizes.max(0)[0].tolist()\n        outputs_masks = outputs[\"pred_masks\"].squeeze(2)\n        outputs_masks = F.interpolate(outputs_masks, size=(max_h, max_w), mode=\"bilinear\", align_corners=False)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "documentation": {}
    },
    {
        "label": "PostProcessPanoptic",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "peekOfCode": "class PostProcessPanoptic(nn.Module):\n    \"\"\"This class converts the output of the model to the final panoptic result, in the format expected by the\n    coco panoptic API \"\"\"\n    def __init__(self, is_thing_map, threshold=0.85):\n        \"\"\"\n        Parameters:\n           is_thing_map: This is a whose keys are the class ids, and the values a boolean indicating whether\n                          the class is  a thing (True) or a stuff (False) class\n           threshold: confidence threshold: segments with confidence lower than this will be deleted\n        \"\"\"",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "documentation": {}
    },
    {
        "label": "dice_loss",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "peekOfCode": "def dice_loss(inputs, targets, num_boxes):\n    \"\"\"\n    Compute the DICE loss, similar to generalized IOU for masks\n    Args:\n        inputs: A float tensor of arbitrary shape.\n                The predictions for each example.\n        targets: A float tensor with the same shape as inputs. Stores the binary\n                 classification label for each element in inputs\n                (0 for the negative class and 1 for the positive class).\n    \"\"\"",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "documentation": {}
    },
    {
        "label": "sigmoid_focal_loss",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "peekOfCode": "def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n    \"\"\"\n    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n    Args:\n        inputs: A float tensor of arbitrary shape.\n                The predictions for each example.\n        targets: A float tensor with the same shape as inputs. Stores the binary\n                 classification label for each element in inputs\n                (0 for the negative class and 1 for the positive class).\n        alpha: (optional) Weighting factor in range (0,1) to balance",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "class Mlp(nn.Module):\n    \"\"\"Multilayer perceptron.\"\"\"\n    def __init__(\n        self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.0\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "WindowAttention",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "class WindowAttention(nn.Module):\n    \"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "SwinTransformerBlock",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "class SwinTransformerBlock(nn.Module):\n    \"\"\"Swin Transformer Block.\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "PatchMerging",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "class PatchMerging(nn.Module):\n    \"\"\"Patch Merging Layer\n    Args:\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n    def __init__(self, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "BasicLayer",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "class BasicLayer(nn.Module):\n    \"\"\"A basic Swin Transformer layer for one stage.\n    Args:\n        dim (int): Number of feature channels\n        depth (int): Depths of this stage.\n        num_heads (int): Number of attention head.\n        window_size (int): Local window size. Default: 7.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\n    Args:\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "SwinTransformer",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "class SwinTransformer(nn.Module):\n    \"\"\"Swin Transformer backbone.\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n    Args:\n        pretrain_img_size (int): Input image size for training the pretrained model,\n            used in absolute postion embedding. Default 224.\n        patch_size (int | tuple(int)): Patch size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "get_swinl",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "def get_swinl(**add_kwargs):\n    model = SwinTransformer(**swin_l_kwargs, **add_kwargs)\n    state_dict = torch.load(swin_l_weights, map_location=torch.device('cpu'))\n    load_info = model.load_state_dict(state_dict['model'], strict=False,)\n    print('Missing swin keys', load_info.missing_keys)\n    print('Unexpected swin keys', load_info.unexpected_keys)\n    return model\nclass Mlp(nn.Module):\n    \"\"\"Multilayer perceptron.\"\"\"\n    def __init__(",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "window_partition",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "def window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "window_reverse",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "def window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "swin_l_weights",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "swin_l_weights = '/home/user/workspace/swarmone/converted_deta_swin_o365_finetune.pth'\ndef get_swinl(**add_kwargs):\n    model = SwinTransformer(**swin_l_kwargs, **add_kwargs)\n    state_dict = torch.load(swin_l_weights, map_location=torch.device('cpu'))\n    load_info = model.load_state_dict(state_dict['model'], strict=False,)\n    print('Missing swin keys', load_info.missing_keys)\n    print('Unexpected swin keys', load_info.unexpected_keys)\n    return model\nclass Mlp(nn.Module):\n    \"\"\"Multilayer perceptron.\"\"\"",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.tools.launch",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.tools.launch",
        "peekOfCode": "def parse_args():\n    \"\"\"\n    Helper function parsing the command line options\n    @retval ArgumentParser\n    \"\"\"\n    parser = ArgumentParser(description=\"PyTorch distributed training launch \"\n                                        \"helper utilty that will spawn up \"\n                                        \"multiple distributed processes\")\n    # Optional arguments for the launch helper\n    parser.add_argument(\"--nnodes\", type=int, default=1,",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.tools.launch",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.tools.launch",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.tools.launch",
        "peekOfCode": "def main():\n    args = parse_args()\n    # world size in terms of number of processes\n    dist_world_size = args.nproc_per_node * args.nnodes\n    # set PyTorch distributed related environmental variables\n    current_env = os.environ.copy()\n    current_env[\"MASTER_ADDR\"] = args.master_addr\n    current_env[\"MASTER_PORT\"] = str(args.master_port)\n    current_env[\"WORLD_SIZE\"] = str(dist_world_size)\n    processes = []",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.tools.launch",
        "documentation": {}
    },
    {
        "label": "box_cxcywh_to_xyxy",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "peekOfCode": "def box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(-1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=-1)\ndef box_xyxy_to_cxcywh(x):\n    x0, y0, x1, y1 = x.unbind(-1)\n    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n         (x1 - x0), (y1 - y0)]\n    return torch.stack(b, dim=-1)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_xyxy_to_cxcywh",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "peekOfCode": "def box_xyxy_to_cxcywh(x):\n    x0, y0, x1, y1 = x.unbind(-1)\n    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n         (x1 - x0), (y1 - y0)]\n    return torch.stack(b, dim=-1)\n# modified from torchvision to also return the union\ndef box_iou(boxes1, boxes2):\n    area1 = box_area(boxes1)\n    area2 = box_area(boxes2)\n    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_iou",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "peekOfCode": "def box_iou(boxes1, boxes2):\n    area1 = box_area(boxes1)\n    area2 = box_area(boxes2)\n    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n    union = area1[:, None] + area2 - inter\n    iou = inter / union\n    return iou, union",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "documentation": {}
    },
    {
        "label": "generalized_box_iou",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "peekOfCode": "def generalized_box_iou(boxes1, boxes2):\n    \"\"\"\n    Generalized IoU from https://giou.stanford.edu/\n    The boxes should be in [x0, y0, x1, y1] format\n    Returns a [N, M] pairwise matrix, where N = len(boxes1)\n    and M = len(boxes2)\n    \"\"\"\n    # degenerate boxes gives inf / nan results\n    # so do an early check\n    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "documentation": {}
    },
    {
        "label": "masks_to_boxes",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "peekOfCode": "def masks_to_boxes(masks):\n    \"\"\"Compute the bounding boxes around the provided masks\n    The masks should be in format [N, H, W] where N is the number of masks, (H, W) are the spatial dimensions.\n    Returns a [N, 4] tensors, with the boxes in xyxy format\n    \"\"\"\n    if masks.numel() == 0:\n        return torch.zeros((0, 4), device=masks.device)\n    h, w = masks.shape[-2:]\n    y = torch.arange(0, h, dtype=torch.float)\n    x = torch.arange(0, w, dtype=torch.float)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "documentation": {}
    },
    {
        "label": "SmoothedValue",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "class SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "MetricLogger",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "class MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\"):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "class NestedTensor(object):\n    def __init__(self, tensors, mask: Optional[Tensor]):\n        self.tensors = tensors\n        self.mask = mask\n    def to(self, device, non_blocking=False):\n        # type: (Device) -> NestedTensor # noqa\n        cast_tensor = self.tensors.to(device, non_blocking=non_blocking)\n        mask = self.mask\n        if mask is not None:\n            assert mask is not None",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "all_gather",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def all_gather(data):\n    \"\"\"\n    Run all_gather on arbitrary picklable data (not necessarily tensors)\n    Args:\n        data: any picklable object\n    Returns:\n        list[data]: list of data gathered from each rank\n    \"\"\"\n    world_size = get_world_size()\n    if world_size == 1:",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "reduce_dict",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def reduce_dict(input_dict, average=True):\n    \"\"\"\n    Args:\n        input_dict (dict): all the values will be reduced\n        average (bool): whether to do average or sum\n    Reduce the values in the dictionary from all processes so that all processes\n    have the averaged results. Returns a dict with the same fields as\n    input_dict, after reduction.\n    \"\"\"\n    world_size = get_world_size()",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "get_sha",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def get_sha():\n    cwd = os.path.dirname(os.path.abspath(__file__))\n    def _run(command):\n        return subprocess.check_output(command, cwd=cwd).decode('ascii').strip()\n    sha = 'N/A'\n    diff = \"clean\"\n    branch = 'N/A'\n    try:\n        sha = _run(['git', 'rev-parse', 'HEAD'])\n        subprocess.check_output(['git', 'diff'], cwd=cwd)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "collate_fn",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def collate_fn(batch):\n    batch = list(zip(*batch))\n    return tuple(batch)\ndef _max_by_axis(the_list):\n    # type: (List[List[int]]) -> List[int]\n    maxes = the_list[0]\n    for sublist in the_list[1:]:\n        for index, item in enumerate(sublist):\n            maxes[index] = max(maxes[index], item)\n    return maxes",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "nested_tensor_from_tensor_list",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n    # TODO make this more general\n    if tensor_list[0].ndim == 3:\n        # TODO make it support different-sized images\n        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n        batch_shape = [len(tensor_list)] + max_size\n        b, c, h, w = batch_shape\n        dtype = tensor_list[0].dtype\n        device = tensor_list[0].device",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "setup_for_distributed",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def setup_for_distributed(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n    def print(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "is_dist_avail_and_initialized",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "get_world_size",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()\ndef get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\ndef get_local_size():\n    if not is_dist_avail_and_initialized():",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "get_rank",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\ndef get_local_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return int(os.environ['LOCAL_SIZE'])\ndef get_local_rank():\n    if not is_dist_avail_and_initialized():",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "get_local_size",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def get_local_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return int(os.environ['LOCAL_SIZE'])\ndef get_local_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return int(os.environ['LOCAL_RANK'])\ndef is_main_process():\n    return get_rank() == 0",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "get_local_rank",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def get_local_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return int(os.environ['LOCAL_RANK'])\ndef is_main_process():\n    return get_rank() == 0\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\ndef init_distributed_mode(args):",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "is_main_process",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def is_main_process():\n    return get_rank() == 0\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\ndef init_distributed_mode(args):\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "save_on_master",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\ndef init_distributed_mode(args):\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])\n        args.dist_url = 'env://'\n        os.environ['LOCAL_SIZE'] = str(torch.cuda.device_count())",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "init_distributed_mode",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def init_distributed_mode(args):\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])\n        args.dist_url = 'env://'\n        os.environ['LOCAL_SIZE'] = str(torch.cuda.device_count())\n    elif 'SLURM_PROCID' in os.environ:\n        proc_id = int(os.environ['SLURM_PROCID'])\n        ntasks = int(os.environ['SLURM_NTASKS'])",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n    if target.numel() == 0:\n        return [torch.zeros([], device=output.device)]\n    maxk = max(topk)\n    batch_size = target.size(0)\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n    res = []",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def interpolate(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n    # type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool]) -> Tensor\n    \"\"\"\n    Equivalent to nn.functional.interpolate, but with support for empty batch sizes.\n    This will eventually be supported natively by PyTorch, and this\n    class can go away.\n    \"\"\"\n    if float(torchvision.__version__[:3]) < 0.7:\n        if input.numel() > 0:\n            return torch.nn.functional.interpolate(",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "get_total_grad_norm",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def get_total_grad_norm(parameters, norm_type=2):\n    parameters = list(filter(lambda p: p.grad is not None, parameters))\n    norm_type = float(norm_type)\n    device = parameters[0].grad.device\n    total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]),\n                            norm_type)\n    return total_norm\ndef inverse_sigmoid(x, eps=1e-5):\n    x = x.clamp(min=0, max=1)\n    x1 = x.clamp(min=eps)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "inverse_sigmoid",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def inverse_sigmoid(x, eps=1e-5):\n    x = x.clamp(min=0, max=1)\n    x1 = x.clamp(min=eps)\n    x2 = (1 - x).clamp(min=eps)\n    return torch.log(x1 / x2)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "plot_logs",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.plot_utils",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.plot_utils",
        "peekOfCode": "def plot_logs(logs, fields=('class_error', 'loss_bbox_unscaled', 'mAP'), ewm_col=0, log_name='log.txt'):\n    '''\n    Function to plot specific fields from training log(s). Plots both training and test results.\n    :: Inputs - logs = list containing Path objects, each pointing to individual dir with a log file\n              - fields = which results to plot from each log file - plots both training and test for each field.\n              - ewm_col = optional, which column to use as the exponential weighted smoothing of the plots\n              - log_name = optional, name of log file if different than default 'log.txt'.\n    :: Outputs - matplotlib plots of results in fields, color coded for each log file.\n               - solid lines are training results, dashed lines are test results.\n    '''",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.plot_utils",
        "documentation": {}
    },
    {
        "label": "plot_precision_recall",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.plot_utils",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.plot_utils",
        "peekOfCode": "def plot_precision_recall(files, naming_scheme='iter'):\n    if naming_scheme == 'exp_id':\n        # name becomes exp_id\n        names = [f.parts[-3] for f in files]\n    elif naming_scheme == 'iter':\n        names = [f.stem for f in files]\n    else:\n        raise ValueError(f'not supported {naming_scheme}')\n    fig, axs = plt.subplots(ncols=2, figsize=(16, 5))\n    for f, color, name in zip(files, sns.color_palette(\"Blues\", n_colors=len(files)), names):",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.util.plot_utils",
        "documentation": {}
    },
    {
        "label": "get_benckmark_arg_parser",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.benchmark",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.benchmark",
        "peekOfCode": "def get_benckmark_arg_parser():\n    parser = argparse.ArgumentParser('Benchmark inference speed of Deformable DETR.')\n    parser.add_argument('--num_iters', type=int, default=300, help='total iters to benchmark speed')\n    parser.add_argument('--warm_iters', type=int, default=5, help='ignore first several iters that are very slow')\n    parser.add_argument('--batch_size', type=int, default=1, help='batch size in inference')\n    parser.add_argument('--resume', type=str, help='load the pre-trained checkpoint')\n    return parser\n@torch.no_grad()\ndef measure_average_inference_time(model, inputs, num_iters=100, warm_iters=5):\n    ts = []",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.benchmark",
        "documentation": {}
    },
    {
        "label": "measure_average_inference_time",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.benchmark",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.benchmark",
        "peekOfCode": "def measure_average_inference_time(model, inputs, num_iters=100, warm_iters=5):\n    ts = []\n    for iter_ in range(num_iters):\n        torch.cuda.synchronize()\n        t_ = time.perf_counter()\n        model(inputs)\n        torch.cuda.synchronize()\n        t = time.perf_counter() - t_\n        if iter_ >= warm_iters:\n          ts.append(t)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.benchmark",
        "documentation": {}
    },
    {
        "label": "benchmark",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.benchmark",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.benchmark",
        "peekOfCode": "def benchmark():\n    args, _ = get_benckmark_arg_parser().parse_known_args()\n    main_args = get_main_args_parser().parse_args(_)\n    assert args.warm_iters < args.num_iters and args.num_iters > 0 and args.warm_iters >= 0\n    assert args.batch_size > 0\n    assert args.resume is None or os.path.exists(args.resume)\n    dataset = build_dataset('val', main_args)\n    model, _, _ = build_model(main_args)\n    model.cuda()\n    model.eval()",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.benchmark",
        "documentation": {}
    },
    {
        "label": "train_one_epoch",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.engine",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.engine",
        "peekOfCode": "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n                    device: torch.device, epoch: int, max_norm: float = 0):\n    model.train()\n    criterion.train()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n    metric_logger.add_meter('grad_norm', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n    header = 'Epoch: [{}]'.format(epoch)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.engine",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.engine",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.engine",
        "peekOfCode": "def evaluate(model, criterion, postprocessors, data_loader, base_ds, device, output_dir):\n    model.eval()\n    criterion.eval()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n    header = 'Test:'\n    iou_types = tuple(k for k in ('segm', 'bbox') if k in postprocessors.keys())\n    coco_evaluator = CocoEvaluator(base_ds, iou_types)\n    # coco_evaluator.coco_eval[iou_types[0]].params.iouThrs = [0, 0.1, 0.5, 0.75]\n    panoptic_evaluator = None",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.engine",
        "documentation": {}
    },
    {
        "label": "DETAModel",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "class DETAModel(L.LightningModule):\n    def __init__(self, model, criterion, postprocessors, base_ds, args):\n        super().__init__()\n        self.model = model\n        self.criterion = criterion\n        self.postprocessors = postprocessors\n        self.base_ds = base_ds\n        self.args = args\n    def forward(self, x):\n        return self.model(x)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "def get_args_parser():\n    parser = argparse.ArgumentParser('Deformable DETR Detector', add_help=False)\n    parser.add_argument('--lr', default=5e-5, type=float)\n    parser.add_argument('--lr_backbone_names', default=[\"backbone.0\"], type=str, nargs='+')\n    parser.add_argument('--lr_backbone', default=5e-6, type=float)\n    parser.add_argument('--lr_linear_proj_names', default=['reference_points', 'sampling_offsets'], type=str, nargs='+')\n    parser.add_argument('--lr_linear_proj_mult', default=0.1, type=float)\n    parser.add_argument('--batch_size', default=2, type=int)\n    parser.add_argument('--weight_decay', default=1e-4, type=float)\n    parser.add_argument('--epochs', default=24, type=int)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "match_name_keywords",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "def match_name_keywords(n, name_keywords):\n    out = False\n    for b in name_keywords:\n        if b in n:\n            out = True\n            break\n    return out\nif args.dataset_file == \"coco_panoptic\":\n    # We also evaluate AP during panoptic training, on original coco DS\n    coco_val = datasets.coco.build(\"val\", args)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "swarm_one_client",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "swarm_one_client = Client(api_key=\"bSNxj8I21w\")\n# pip install torchvision==0.16.0 torch==2.1.0 timm\ndef get_args_parser():\n    parser = argparse.ArgumentParser('Deformable DETR Detector', add_help=False)\n    parser.add_argument('--lr', default=5e-5, type=float)\n    parser.add_argument('--lr_backbone_names', default=[\"backbone.0\"], type=str, nargs='+')\n    parser.add_argument('--lr_backbone', default=5e-6, type=float)\n    parser.add_argument('--lr_linear_proj_names', default=['reference_points', 'sampling_offsets'], type=str, nargs='+')\n    parser.add_argument('--lr_linear_proj_mult', default=0.1, type=float)\n    parser.add_argument('--batch_size', default=2, type=int)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "parser = argparse.ArgumentParser('Deformable DETR training and evaluation script', parents=[get_args_parser()])\n## Custom args\ncustom_args = '--with_box_refine --two_stage \\\n    --num_feature_levels 5 --num_queries 900 \\\n    --dim_feedforward 2048 --dropout 0.0 --cls_loss_coef 1.0 \\\n    --assign_first_stage --assign_second_stage \\\n    --epochs 24 --lr_drop 20 \\\n    --lr 5e-5 --lr_backbone 5e-6 --batch_size 1 \\\n    --backbone swin \\\n    --bigger'.split()",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "custom_args",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "custom_args = '--with_box_refine --two_stage \\\n    --num_feature_levels 5 --num_queries 900 \\\n    --dim_feedforward 2048 --dropout 0.0 --cls_loss_coef 1.0 \\\n    --assign_first_stage --assign_second_stage \\\n    --epochs 24 --lr_drop 20 \\\n    --lr 5e-5 --lr_backbone 5e-6 --batch_size 1 \\\n    --backbone swin \\\n    --bigger'.split()\nargs = parser.parse_args(custom_args)\nmodel, criterion, postprocessors = build_model(args)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "args = parser.parse_args(custom_args)\nmodel, criterion, postprocessors = build_model(args)\ndataset_train = build_dataset(image_set='train', args=args)\ndataset_val = build_dataset(image_set='val', args=args)\ndata_loader_train = DataLoader(dataset_train, 2, shuffle=True,\n                               collate_fn=utils.collate_fn, num_workers=0,\n                               pin_memory=True)\ndata_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "dataset_train",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "dataset_train = build_dataset(image_set='train', args=args)\ndataset_val = build_dataset(image_set='val', args=args)\ndata_loader_train = DataLoader(dataset_train, 2, shuffle=True,\n                               collate_fn=utils.collate_fn, num_workers=0,\n                               pin_memory=True)\ndata_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)\ndef match_name_keywords(n, name_keywords):\n    out = False",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "dataset_val",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "dataset_val = build_dataset(image_set='val', args=args)\ndata_loader_train = DataLoader(dataset_train, 2, shuffle=True,\n                               collate_fn=utils.collate_fn, num_workers=0,\n                               pin_memory=True)\ndata_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)\ndef match_name_keywords(n, name_keywords):\n    out = False\n    for b in name_keywords:",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "data_loader_train",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "data_loader_train = DataLoader(dataset_train, 2, shuffle=True,\n                               collate_fn=utils.collate_fn, num_workers=0,\n                               pin_memory=True)\ndata_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)\ndef match_name_keywords(n, name_keywords):\n    out = False\n    for b in name_keywords:\n        if b in n:",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "data_loader_val",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "data_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)\ndef match_name_keywords(n, name_keywords):\n    out = False\n    for b in name_keywords:\n        if b in n:\n            out = True\n            break\n    return out",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "model = DETAModel(model, criterion, postprocessors, base_ds, args)\nhyperparameters = {\n    \"max_epochs\": [50],\n    \"batch_sizes\": [8],\n}\n# job_id = swarm_one_client.fit(\n#     model='FU3HsIkF',\n#     train_dataloaders=data_loader_val,\n#     val_dataloaders=data_loader_val,\n#     hyperparameters=hyperparameters",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "hyperparameters",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "hyperparameters = {\n    \"max_epochs\": [50],\n    \"batch_sizes\": [8],\n}\n# job_id = swarm_one_client.fit(\n#     model='FU3HsIkF',\n#     train_dataloaders=data_loader_val,\n#     val_dataloaders=data_loader_val,\n#     hyperparameters=hyperparameters\n# )",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "job_id",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "job_id = '76RP82oY'\nloggin_dir = f\"/home/user/workspace/swarmone/swarm_poc/data/tf_log/{job_id}\"\nfrom logging_swarmone_clearml import log_job_metrics\nlog_job_metrics(job_id, \"DETA\", swarm_one_client)\n# swarm_one_client.download_tensorboard_logs(job_id, log_dir=loggin_dir, show_tensorboard=False)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "loggin_dir",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "loggin_dir = f\"/home/user/workspace/swarmone/swarm_poc/data/tf_log/{job_id}\"\nfrom logging_swarmone_clearml import log_job_metrics\nlog_job_metrics(job_id, \"DETA\", swarm_one_client)\n# swarm_one_client.download_tensorboard_logs(job_id, log_dir=loggin_dir, show_tensorboard=False)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "DETAModel",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "class DETAModel(L.LightningModule):\n    def __init__(self, model, criterion, postprocessors, args):\n        super().__init__()\n        self.model = model\n        self.criterion = criterion\n        self.postprocessors = postprocessors\n        self.args = args\n    def configure_optimizers(self):\n        param_dicts = [\n            {",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "def get_args_parser():\n    parser = argparse.ArgumentParser('Deformable DETR Detector', add_help=False)\n    parser.add_argument('--lr', default=5e-5, type=float)\n    parser.add_argument('--lr_backbone_names', default=[\"backbone.0\"], type=str, nargs='+')\n    parser.add_argument('--lr_backbone', default=5e-6, type=float)\n    parser.add_argument('--lr_linear_proj_names', default=['reference_points', 'sampling_offsets'], type=str, nargs='+')\n    parser.add_argument('--lr_linear_proj_mult', default=0.1, type=float)\n    parser.add_argument('--batch_size', default=2, type=int)\n    parser.add_argument('--weight_decay', default=1e-4, type=float)\n    parser.add_argument('--epochs', default=24, type=int)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "match_name_keywords",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "def match_name_keywords(n, name_keywords):\n    out = False\n    for b in name_keywords:\n        if b in n:\n            out = True\n            break\n    return out\nif args.dataset_file == \"coco_panoptic\":\n    # We also evaluate AP during panoptic training, on original coco DS\n    coco_val = datasets.coco.build(\"val\", args)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "def main(args):\n    model = DETAModel(model=model, criterion=criterion, postprocessors=postprocessors, args=args)\n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_loss',  \n        dirpath='./model_checkpoints/',\n        filename='deformable-detr-{epoch:02d}-{val_loss:.2f}',\n        save_top_k=3,  # Save the top 3 models according to the monitored metric\n        mode='min',  # 'min' mode saves models with the lowest 'monitor' metric\n    )\n    dataset_train = build_dataset(image_set='train', args=args)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "parser = argparse.ArgumentParser('Deformable DETR training and evaluation script', parents=[get_args_parser()])\ncustom_args = '--with_box_refine --two_stage \\\n    --num_feature_levels 5 --num_queries 900 \\\n    --dim_feedforward 2048 --dropout 0.0 --cls_loss_coef 1.0 \\\n    --assign_first_stage --assign_second_stage \\\n    --epochs 24 --lr_drop 20 \\\n    --lr 5e-5 --lr_backbone 5e-6 --batch_size 1 \\\n    --backbone swin \\\n    --bigger'.split()\nargs = parser.parse_args(custom_args)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "custom_args",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "custom_args = '--with_box_refine --two_stage \\\n    --num_feature_levels 5 --num_queries 900 \\\n    --dim_feedforward 2048 --dropout 0.0 --cls_loss_coef 1.0 \\\n    --assign_first_stage --assign_second_stage \\\n    --epochs 24 --lr_drop 20 \\\n    --lr 5e-5 --lr_backbone 5e-6 --batch_size 1 \\\n    --backbone swin \\\n    --bigger'.split()\nargs = parser.parse_args(custom_args)\nmodel, criterion, postprocessors = build_model(args)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "args = parser.parse_args(custom_args)\nmodel, criterion, postprocessors = build_model(args)\ndataset_train = build_dataset(image_set='train', args=args)\ndataset_val = build_dataset(image_set='val', args=args)\ndata_loader_train = DataLoader(dataset_train, 2, shuffle=True,\n                               collate_fn=utils.collate_fn, num_workers=0,\n                               pin_memory=True)\ndata_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "dataset_train",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "dataset_train = build_dataset(image_set='train', args=args)\ndataset_val = build_dataset(image_set='val', args=args)\ndata_loader_train = DataLoader(dataset_train, 2, shuffle=True,\n                               collate_fn=utils.collate_fn, num_workers=0,\n                               pin_memory=True)\ndata_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)\ndef match_name_keywords(n, name_keywords):\n    out = False",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "dataset_val",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "dataset_val = build_dataset(image_set='val', args=args)\ndata_loader_train = DataLoader(dataset_train, 2, shuffle=True,\n                               collate_fn=utils.collate_fn, num_workers=0,\n                               pin_memory=True)\ndata_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)\ndef match_name_keywords(n, name_keywords):\n    out = False\n    for b in name_keywords:",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "data_loader_train",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "data_loader_train = DataLoader(dataset_train, 2, shuffle=True,\n                               collate_fn=utils.collate_fn, num_workers=0,\n                               pin_memory=True)\ndata_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)\ndef match_name_keywords(n, name_keywords):\n    out = False\n    for b in name_keywords:\n        if b in n:",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "data_loader_val",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "data_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)\ndef match_name_keywords(n, name_keywords):\n    out = False\n    for b in name_keywords:\n        if b in n:\n            out = True\n            break\n    return out",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "model = DETAModel(model=model,\n                  criterion=criterion,\n                  postprocessors=postprocessors,\n                  args=args)\nhyperparameters = {\n    \"max_epochs\": [5],\n    \"batch_sizes\": [16],\n}\ndef main(args):\n    model = DETAModel(model=model, criterion=criterion, postprocessors=postprocessors, args=args)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "hyperparameters",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "hyperparameters = {\n    \"max_epochs\": [5],\n    \"batch_sizes\": [16],\n}\ndef main(args):\n    model = DETAModel(model=model, criterion=criterion, postprocessors=postprocessors, args=args)\n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_loss',  \n        dirpath='./model_checkpoints/',\n        filename='deformable-detr-{epoch:02d}-{val_loss:.2f}',",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "ContrastiveTransformations",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "class ContrastiveTransformations:\n    def __init__(self, base_transforms, n_views=2):\n        self.base_transforms = base_transforms\n        self.n_views = n_views\n    def __call__(self, x):\n        return [self.base_transforms(x) for i in range(self.n_views)]\ncontrast_transforms = transforms.Compose(\n    [\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomResizedCrop(size=224),",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "DinoV2Model",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "class DinoV2Model(L.LightningModule):\n    def __init__(self, learning_rate):\n        super().__init__()\n        # self.model = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vitg14\", pretrained=True)\n        self.model = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\", pretrained=True)\n        self.lr = learning_rate\n        self.temperature = 0.07\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=1e-4)\n        return optimizer",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "create_subset",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "def create_subset(dataset, subset_size=4096):\n    total_samples = len(dataset)\n    indices = np.random.choice(range(total_samples), subset_size, replace=True)\n    return Subset(dataset, indices)\nunlabeled_data_full = STL10(\n    root=DATASET_PATH,\n    split=\"unlabeled\",\n    download=False,\n    transform=ContrastiveTransformations(contrast_transforms, n_views=2),\n)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "swarm_one_client",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "swarm_one_client = Client(api_key=\"API_KEY\")\n# ## Imports 📦\nimport lightning as L\nimport numpy as np\nimport os\nimport torch\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nfrom torchvision.datasets import STL10\nfrom torch.utils.data import DataLoader, Subset",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "DATASET_PATH",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"../stl10_data/\")\n# ## Contrastive Transformations\nclass ContrastiveTransformations:\n    def __init__(self, base_transforms, n_views=2):\n        self.base_transforms = base_transforms\n        self.n_views = n_views\n    def __call__(self, x):\n        return [self.base_transforms(x) for i in range(self.n_views)]\ncontrast_transforms = transforms.Compose(\n    [",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "contrast_transforms",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "contrast_transforms = transforms.Compose(\n    [\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomResizedCrop(size=224),\n        transforms.RandomApply([transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)], p=0.8),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.GaussianBlur(kernel_size=9),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,)),\n    ]",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "unlabeled_data_full",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "unlabeled_data_full = STL10(\n    root=DATASET_PATH,\n    split=\"unlabeled\",\n    download=False,\n    transform=ContrastiveTransformations(contrast_transforms, n_views=2),\n)\ntrain_data_contrast_full = STL10(\n    root=DATASET_PATH,\n    split=\"train\",\n    download=False,",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "train_data_contrast_full",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "train_data_contrast_full = STL10(\n    root=DATASET_PATH,\n    split=\"train\",\n    download=False,\n    transform=ContrastiveTransformations(contrast_transforms, n_views=2),\n)\nunlabeled_data = create_subset(unlabeled_data_full, subset_size=9000)\ntrain_data_contrast = create_subset(train_data_contrast_full, subset_size=1000)\n# ## DataLoaders\ntrain_dataloader = DataLoader(",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "unlabeled_data",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "unlabeled_data = create_subset(unlabeled_data_full, subset_size=9000)\ntrain_data_contrast = create_subset(train_data_contrast_full, subset_size=1000)\n# ## DataLoaders\ntrain_dataloader = DataLoader(\n    unlabeled_data,\n    batch_size=64,\n    shuffle=True,\n    drop_last=True,\n    pin_memory=True\n)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "train_data_contrast",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "train_data_contrast = create_subset(train_data_contrast_full, subset_size=1000)\n# ## DataLoaders\ntrain_dataloader = DataLoader(\n    unlabeled_data,\n    batch_size=64,\n    shuffle=True,\n    drop_last=True,\n    pin_memory=True\n)\nval_dataloader = DataLoader(",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "train_dataloader",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "train_dataloader = DataLoader(\n    unlabeled_data,\n    batch_size=64,\n    shuffle=True,\n    drop_last=True,\n    pin_memory=True\n)\nval_dataloader = DataLoader(\n    train_data_contrast,\n    batch_size=64,",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "val_dataloader",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "val_dataloader = DataLoader(\n    train_data_contrast,\n    batch_size=64,\n    shuffle=False,\n    drop_last=False,\n    pin_memory=True\n)\n# ## DinoV2 LightningModule\nclass DinoV2Model(L.LightningModule):\n    def __init__(self, learning_rate):",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "model = DinoV2Model(learning_rate=0.01)\n# ## Training ⚡⚡⚡\n# You can abort any **JOB** / **TASK** at anytime to reduce your costs & total training time - from **your dashboard**\njob_id = swarm_one_client.fit(\n    model=model,\n    train_dataloaders=train_dataloader,\n    val_dataloaders=val_dataloader,\n    hyperparameters={\n        \"max_epochs\": [10],\n        \"batch_sizes\": [2048],",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "job_id",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "job_id = swarm_one_client.fit(\n    model=model,\n    train_dataloaders=train_dataloader,\n    val_dataloaders=val_dataloader,\n    hyperparameters={\n        \"max_epochs\": [10],\n        \"batch_sizes\": [2048],\n    },\n    name=f'AutoBrains Dino V2 Giant'\n)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "trained_model",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "trained_model = swarm_one_client.download_trained_model(\"TASK_ID\", \".\")\nprint(trained_model)\nhistory = swarm_one_client.get_task_history(\"TASK_ID\")\nprint(history)\njob_history = swarm_one_client.get_job_history(job_id=job_id, to_pandas=True)\nprint(job_history)\njob_info = swarm_one_client.get_job_information(job_id, to_pandas=True)\nprint(job_info)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "history",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "history = swarm_one_client.get_task_history(\"TASK_ID\")\nprint(history)\njob_history = swarm_one_client.get_job_history(job_id=job_id, to_pandas=True)\nprint(job_history)\njob_info = swarm_one_client.get_job_information(job_id, to_pandas=True)\nprint(job_info)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "job_history",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "job_history = swarm_one_client.get_job_history(job_id=job_id, to_pandas=True)\nprint(job_history)\njob_info = swarm_one_client.get_job_information(job_id, to_pandas=True)\nprint(job_info)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "job_info",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "job_info = swarm_one_client.get_job_information(job_id, to_pandas=True)\nprint(job_info)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "CustomDataset",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "class CustomDataset(Dataset):\n    def __init__(self, post_shape, split, num_samples=100):\n        self.post_shape = post_shape[1:]\n        self.num_samples = num_samples\n        self.cifar10 = datasets.CIFAR10(root=\"../data\", train=(split == \"train\"), download=False)\n        if split == \"train\":\n            transform_list = []\n            transform_list.extend([\n                v2.RandomHorizontalFlip(),\n                v2.RandomVerticalFlip(),",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "MobileNetV2",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "class MobileNetV2(L.LightningModule):\n    def __init__(self, num_classes, optimizer_type, learning_rate):\n        super().__init__()\n        self.model = mobilenet_v2(pretrained=True)\n        self.model.classifier = torch.nn.Sequential(torch.nn.Linear(self.model.last_channel, num_classes))\n        self.criterion = nn.CrossEntropyLoss()\n        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n        self.learning_rate = learning_rate\n        self.optimizer_type = optimizer_type\n    def forward(self, x):",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "swarm_one_client",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "swarm_one_client = Client(api_key=\"API_KEY\")\n# ## Imports 📦\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchmetrics\nimport lightning as L\nimport numpy as np\nfrom torchvision.models import mobilenet_v2\nfrom torchvision.transforms import Compose",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "train_dataset = CustomDataset(post_shape=(3, 224, 224),\n                              split=\"train\",\n                              num_samples=270_000)\nval_dataset = CustomDataset(post_shape=(3, 224, 224),\n                            split=\"val\",\n                            num_samples=30_000)\n# ## DataLoaders\ntrain_dataloader = DataLoader(train_dataset,\n                              batch_size=64,\n                              shuffle=True)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "val_dataset = CustomDataset(post_shape=(3, 224, 224),\n                            split=\"val\",\n                            num_samples=30_000)\n# ## DataLoaders\ntrain_dataloader = DataLoader(train_dataset,\n                              batch_size=64,\n                              shuffle=True)\nval_dataloader = DataLoader(val_dataset,\n                            batch_size=64,\n                            shuffle=False)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "train_dataloader",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "train_dataloader = DataLoader(train_dataset,\n                              batch_size=64,\n                              shuffle=True)\nval_dataloader = DataLoader(val_dataset,\n                            batch_size=64,\n                            shuffle=False)\nclass MobileNetV2(L.LightningModule):\n    def __init__(self, num_classes, optimizer_type, learning_rate):\n        super().__init__()\n        self.model = mobilenet_v2(pretrained=True)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "val_dataloader",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "val_dataloader = DataLoader(val_dataset,\n                            batch_size=64,\n                            shuffle=False)\nclass MobileNetV2(L.LightningModule):\n    def __init__(self, num_classes, optimizer_type, learning_rate):\n        super().__init__()\n        self.model = mobilenet_v2(pretrained=True)\n        self.model.classifier = torch.nn.Sequential(torch.nn.Linear(self.model.last_channel, num_classes))\n        self.criterion = nn.CrossEntropyLoss()\n        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "models",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "models = {\n    \"adam_lr_0.01\": MobileNetV2(num_classes=10, optimizer_type=\"adam\", learning_rate=0.01),\n    \"adam_lr_005\": MobileNetV2(num_classes=10, optimizer_type=\"adam\", learning_rate=0.005),\n    \"adamw_lr_005\": MobileNetV2(num_classes=10, optimizer_type=\"adamW\", learning_rate=0.005),\n}\nhyperparameters = {\n    \"max_epochs\": [10, 20],\n    \"batch_sizes\": [128, 256],\n}\n# ## Training ⚡⚡⚡",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "hyperparameters",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "hyperparameters = {\n    \"max_epochs\": [10, 20],\n    \"batch_sizes\": [128, 256],\n}\n# ## Training ⚡⚡⚡\n# You can abort any **JOB** / **TASK** at anytime to reduce your costs & total training time - from **your dashboard**\njob_id = swarm_one_client.fit(\n    model=models,\n    train_dataloaders=train_dataloader,\n    val_dataloaders=val_dataloader,",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "job_id",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "job_id = swarm_one_client.fit(\n    model=models,\n    train_dataloaders=train_dataloader,\n    val_dataloaders=val_dataloader,\n    hyperparameters=hyperparameters,\n    name=f'AutoBrains MobileNetV2',\n)\n# ## Tensorboard Logs 📉 📈\nswarm_one_client.download_tensorboard_logs(job_id, log_dir=\"path\", show_tensorboard=False)\n# ## Trained Model Downloading 🎯 🏆",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "trained_model",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "trained_model = swarm_one_client.download_trained_model(\"TASK_ID\", \".\")\nprint(trained_model)\nhistory = swarm_one_client.get_task_history(\"TASK_ID\")\nprint(history)\njob_history = swarm_one_client.get_job_history(job_id=job_id, to_pandas=True)\nprint(job_history)\njob_info = swarm_one_client.get_job_information(job_id, to_pandas=True)\nprint(job_info)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "history",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "history = swarm_one_client.get_task_history(\"TASK_ID\")\nprint(history)\njob_history = swarm_one_client.get_job_history(job_id=job_id, to_pandas=True)\nprint(job_history)\njob_info = swarm_one_client.get_job_information(job_id, to_pandas=True)\nprint(job_info)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "job_history",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "job_history = swarm_one_client.get_job_history(job_id=job_id, to_pandas=True)\nprint(job_history)\njob_info = swarm_one_client.get_job_information(job_id, to_pandas=True)\nprint(job_info)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "job_info",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "job_info = swarm_one_client.get_job_information(job_id, to_pandas=True)\nprint(job_info)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "SimpleNN",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "class SimpleNN(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(28 * 28, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 10)\n    def forward(self, x):\n        x = self.flatten(x)\n        x = torch.relu(self.fc1(x))",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "swarm_one_client",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "swarm_one_client = Client(api_key=\"bSNxj8I21w\")\nimport lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n# Define a PyTorch Lightning module\nclass SimpleNN(pl.LightningModule):\n    def __init__(self):\n        super().__init__()",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "transform",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrain_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\nval_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n# Create data loaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [5, 10, 15],",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\nval_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n# Create data loaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [5, 10, 15],\n    \"batch_sizes\": [64]",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "val_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n# Create data loaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [5, 10, 15],\n    \"batch_sizes\": [64]\n}",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "train_loader",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [5, 10, 15],\n    \"batch_sizes\": [64]\n}\n# Train the model\njob_id = swarm_one_client.fit(",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "val_loader",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [5, 10, 15],\n    \"batch_sizes\": [64]\n}\n# Train the model\njob_id = swarm_one_client.fit(\n    model=model,",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "model = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [5, 10, 15],\n    \"batch_sizes\": [64]\n}\n# Train the model\njob_id = swarm_one_client.fit(\n    model=model,\n    train_dataloaders=train_loader,\n    val_dataloaders=val_loader,",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "hyperparameters",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "hyperparameters = {\n    \"max_epochs\": [5, 10, 15],\n    \"batch_sizes\": [64]\n}\n# Train the model\njob_id = swarm_one_client.fit(\n    model=model,\n    train_dataloaders=train_loader,\n    val_dataloaders=val_loader,\n    hyperparameters=hyperparameters,",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "job_id",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "job_id = swarm_one_client.fit(\n    model=model,\n    train_dataloaders=train_loader,\n    val_dataloaders=val_loader,\n    hyperparameters=hyperparameters,\n    name=\"MNIST test\"\n)\nswarm_one_client.download_tensorboard_logs(job_id, log_dir=\"path\", show_tensorboard=False)",
        "detail": "swarm_clean.swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "VERSION",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.swarm_one-0.0.4.setup",
        "description": "swarm_clean.swarm_poc.swarm_one-0.0.4.setup",
        "peekOfCode": "VERSION = '0.0.4'\nDESCRIPTION = ' '\n_CURR_DIRECTORY = os.path.abspath(os.path.dirname(__file__))\nwith open(os.path.join(_CURR_DIRECTORY, \"requirements.txt\")) as f:\n   setup(\n      name=\"swarm_one\",\n      version=VERSION,\n      author=\"Kim Boren\",\n      author_email=\"kimb@r-stealth.com\",\n      description=DESCRIPTION,",
        "detail": "swarm_clean.swarm_poc.swarm_one-0.0.4.setup",
        "documentation": {}
    },
    {
        "label": "DESCRIPTION",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.swarm_one-0.0.4.setup",
        "description": "swarm_clean.swarm_poc.swarm_one-0.0.4.setup",
        "peekOfCode": "DESCRIPTION = ' '\n_CURR_DIRECTORY = os.path.abspath(os.path.dirname(__file__))\nwith open(os.path.join(_CURR_DIRECTORY, \"requirements.txt\")) as f:\n   setup(\n      name=\"swarm_one\",\n      version=VERSION,\n      author=\"Kim Boren\",\n      author_email=\"kimb@r-stealth.com\",\n      description=DESCRIPTION,\n      packages=find_packages(),",
        "detail": "swarm_clean.swarm_poc.swarm_one-0.0.4.setup",
        "documentation": {}
    },
    {
        "label": "_CURR_DIRECTORY",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.swarm_one-0.0.4.setup",
        "description": "swarm_clean.swarm_poc.swarm_one-0.0.4.setup",
        "peekOfCode": "_CURR_DIRECTORY = os.path.abspath(os.path.dirname(__file__))\nwith open(os.path.join(_CURR_DIRECTORY, \"requirements.txt\")) as f:\n   setup(\n      name=\"swarm_one\",\n      version=VERSION,\n      author=\"Kim Boren\",\n      author_email=\"kimb@r-stealth.com\",\n      description=DESCRIPTION,\n      packages=find_packages(),\n      install_requires=f.read().splitlines(),",
        "detail": "swarm_clean.swarm_poc.swarm_one-0.0.4.setup",
        "documentation": {}
    },
    {
        "label": "init_wandb_run",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "description": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "peekOfCode": "def init_wandb_run(task_id, project_name):\n    return wandb.init(project=project_name, name=task_id, id=task_id, resume=\"allow\")\ndef log_metrics(task_id, metrics, epoch=None):\n    global last_logged\n    # Ensure task_id is initialized in last_logged\n    if task_id not in last_logged:\n        last_logged[task_id] = {}\n    for metric, values in metrics.items():\n        # Dynamically add new metrics to last_logged if they don't exist\n        if metric not in last_logged[task_id]:",
        "detail": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "documentation": {}
    },
    {
        "label": "log_metrics",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "description": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "peekOfCode": "def log_metrics(task_id, metrics, epoch=None):\n    global last_logged\n    # Ensure task_id is initialized in last_logged\n    if task_id not in last_logged:\n        last_logged[task_id] = {}\n    for metric, values in metrics.items():\n        # Dynamically add new metrics to last_logged if they don't exist\n        if metric not in last_logged[task_id]:\n            last_logged[task_id][metric] = 0  # Initialize with 0, indicating no values logged yet\n        for i, value in enumerate(values):",
        "detail": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "documentation": {}
    },
    {
        "label": "log_job_metrics",
        "kind": 2,
        "importPath": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "description": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "peekOfCode": "def log_job_metrics(job_id, project_name):\n    \"\"\"\n    Main process to fetch, log, and manage metrics for all tasks within a job.\n    \"\"\"\n    while True:\n        job_info = swarm_one_client.get_job_information(job_id)\n        all_completed = all(info['status'] in ['COMPLETED', 'ABORTED'] for info in job_info.values())\n        for task_id in job_info.keys():\n            wandb_run = init_wandb_run(task_id, project_name)\n            task_metrics = swarm_one_client.get_job_history(job_id, current=True).get(task_id, {})",
        "detail": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "documentation": {}
    },
    {
        "label": "swarm_one_client",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "description": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "peekOfCode": "swarm_one_client = Client(api_key=\"API_KEY\")\nimport os\nimport wandb\nimport time\nos.environ[\"WANDB_DISABLE_SERVICE\"] = \"True\"\nos.environ[\"WANDB_API_KEY\"] = \"YOUR_WANDB_API_KEY\"\nlast_logged = {}\ndef init_wandb_run(task_id, project_name):\n    return wandb.init(project=project_name, name=task_id, id=task_id, resume=\"allow\")\ndef log_metrics(task_id, metrics, epoch=None):",
        "detail": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "documentation": {}
    },
    {
        "label": "os.environ[\"WANDB_DISABLE_SERVICE\"]",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "description": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "peekOfCode": "os.environ[\"WANDB_DISABLE_SERVICE\"] = \"True\"\nos.environ[\"WANDB_API_KEY\"] = \"YOUR_WANDB_API_KEY\"\nlast_logged = {}\ndef init_wandb_run(task_id, project_name):\n    return wandb.init(project=project_name, name=task_id, id=task_id, resume=\"allow\")\ndef log_metrics(task_id, metrics, epoch=None):\n    global last_logged\n    # Ensure task_id is initialized in last_logged\n    if task_id not in last_logged:\n        last_logged[task_id] = {}",
        "detail": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "documentation": {}
    },
    {
        "label": "os.environ[\"WANDB_API_KEY\"]",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "description": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "peekOfCode": "os.environ[\"WANDB_API_KEY\"] = \"YOUR_WANDB_API_KEY\"\nlast_logged = {}\ndef init_wandb_run(task_id, project_name):\n    return wandb.init(project=project_name, name=task_id, id=task_id, resume=\"allow\")\ndef log_metrics(task_id, metrics, epoch=None):\n    global last_logged\n    # Ensure task_id is initialized in last_logged\n    if task_id not in last_logged:\n        last_logged[task_id] = {}\n    for metric, values in metrics.items():",
        "detail": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "documentation": {}
    },
    {
        "label": "last_logged",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "description": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "peekOfCode": "last_logged = {}\ndef init_wandb_run(task_id, project_name):\n    return wandb.init(project=project_name, name=task_id, id=task_id, resume=\"allow\")\ndef log_metrics(task_id, metrics, epoch=None):\n    global last_logged\n    # Ensure task_id is initialized in last_logged\n    if task_id not in last_logged:\n        last_logged[task_id] = {}\n    for metric, values in metrics.items():\n        # Dynamically add new metrics to last_logged if they don't exist",
        "detail": "swarm_clean.swarm_poc.logging_swarmone_wandb",
        "documentation": {}
    },
    {
        "label": "SimpleNN",
        "kind": 6,
        "importPath": "swarm_clean.swarm_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.pytorch_test",
        "peekOfCode": "class SimpleNN(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(28*28, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 10)\n    def forward(self, x):\n        x = self.flatten(x)\n        x = torch.relu(self.fc1(x))",
        "detail": "swarm_clean.swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "swarm_one_client",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.pytorch_test",
        "peekOfCode": "swarm_one_client = Client(api_key=\"API_KEY\")\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport lightning as pl\n# Define a PyTorch Lightning module\nclass SimpleNN(pl.LightningModule):\n    def __init__(self):\n        super().__init__()",
        "detail": "swarm_clean.swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "transform",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.pytorch_test",
        "peekOfCode": "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrain_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\nval_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n# Create data loaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [10, 50, 100],",
        "detail": "swarm_clean.swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.pytorch_test",
        "peekOfCode": "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\nval_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n# Create data loaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [10, 50, 100],\n    \"batch_sizes\": [64]",
        "detail": "swarm_clean.swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.pytorch_test",
        "peekOfCode": "val_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n# Create data loaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [10, 50, 100],\n    \"batch_sizes\": [64]\n}",
        "detail": "swarm_clean.swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "train_loader",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.pytorch_test",
        "peekOfCode": "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [10, 50, 100],\n    \"batch_sizes\": [64]\n}\n# Train the model\njob_id = swarm_one_client.fit(",
        "detail": "swarm_clean.swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "val_loader",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.pytorch_test",
        "peekOfCode": "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [10, 50, 100],\n    \"batch_sizes\": [64]\n}\n# Train the model\njob_id = swarm_one_client.fit(\n    model=model,",
        "detail": "swarm_clean.swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.pytorch_test",
        "peekOfCode": "model = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [10, 50, 100],\n    \"batch_sizes\": [64]\n}\n# Train the model\njob_id = swarm_one_client.fit(\n    model=model,\n    train_dataloaders=train_loader,\n    val_dataloaders=val_loader,",
        "detail": "swarm_clean.swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "hyperparameters",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.pytorch_test",
        "peekOfCode": "hyperparameters = {\n    \"max_epochs\": [10, 50, 100],\n    \"batch_sizes\": [64]\n}\n# Train the model\njob_id = swarm_one_client.fit(\n    model=model,\n    train_dataloaders=train_loader,\n    val_dataloaders=val_loader,\n    hyperparameters=hyperparameters,",
        "detail": "swarm_clean.swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "job_id",
        "kind": 5,
        "importPath": "swarm_clean.swarm_poc.pytorch_test",
        "description": "swarm_clean.swarm_poc.pytorch_test",
        "peekOfCode": "job_id = swarm_one_client.fit(\n    model=model,\n    train_dataloaders=train_loader,\n    val_dataloaders=val_loader,\n    hyperparameters=hyperparameters,\n    name=\"MNIST test\"\n)\nif __name__ == \"__main__\":\n    project_name = 'your_project_name'  # OR the job_id\n    from logging_swarmone_wandb import log_job_metrics",
        "detail": "swarm_clean.swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "CocoDetection",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.torchvision_datasets.coco",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.torchvision_datasets.coco",
        "peekOfCode": "class CocoDetection(VisionDataset):\n    \"\"\"`MS Coco Detection <http://mscoco.org/dataset/#detections-challenge2016>`_ Dataset.\n    Args:\n        root (string): Root directory where images are downloaded to.\n        annFile (string): Path to json annotation file.\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.ToTensor``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        transforms (callable, optional): A function/transform that takes input sample and its target as entry",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.torchvision_datasets.coco",
        "documentation": {}
    },
    {
        "label": "CocoDetection",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "peekOfCode": "class CocoDetection(TvCocoDetection):\n    def __init__(self, img_folder, ann_file, transforms, return_masks, cache_mode=False, local_rank=0, local_size=1):\n        super(CocoDetection, self).__init__(img_folder, ann_file,\n                                            cache_mode=cache_mode, local_rank=local_rank, local_size=local_size)\n        self.prepare = ConvertCocoPolysToMask(return_masks)\n        self._transforms = lambda r: transforms(r[0], r[1])\n        self.swarm_one_transform_name = \"_transforms\"\n    def __getitem__(self, idx):\n        img, target = super(CocoDetection, self).__getitem__(idx)\n        image_id = self.ids[idx]",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "documentation": {}
    },
    {
        "label": "ConvertCocoPolysToMask",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "peekOfCode": "class ConvertCocoPolysToMask(object):\n    def __init__(self, return_masks=False):\n        self.return_masks = return_masks\n    def __call__(self, image, target):\n        w, h = image.size\n        image_id = target[\"image_id\"]\n        image_id = torch.tensor([image_id])\n        anno = target[\"annotations\"]\n        anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n        boxes = [obj[\"bbox\"] for obj in anno]",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "documentation": {}
    },
    {
        "label": "convert_coco_poly_to_mask",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "peekOfCode": "def convert_coco_poly_to_mask(segmentations, height, width):\n    masks = []\n    for polygons in segmentations:\n        rles = coco_mask.frPyObjects(polygons, height, width)\n        mask = coco_mask.decode(rles)\n        if len(mask.shape) < 3:\n            mask = mask[..., None]\n        mask = torch.as_tensor(mask, dtype=torch.uint8)\n        mask = mask.any(dim=2)\n        masks.append(mask)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "documentation": {}
    },
    {
        "label": "make_coco_transforms",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "peekOfCode": "def make_coco_transforms(image_set, bigger):\n    normalize = v2.Compose([\n        v2.ToTensor(),\n        v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    scales = None\n    if 'train' in image_set:\n        scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n    if 'val' in image_set or 'test' in image_set:\n        scales = [800]",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "documentation": {}
    },
    {
        "label": "build",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "peekOfCode": "def build(image_set, args):\n    root = Path(args.coco_path)\n    assert root.exists(), f'provided COCO path {root} does not exist'\n    mode = 'instances'\n    PATHS = {\n        \"train\": (\"/media/DATADISK/8MP_tagged_data\", '/media/DATADISK/coco_datasets/swarm_poc/train_subset.json'),\n        \"val\": (\"/media/DATADISK/8MP_tagged_data\", '/media/DATADISK/coco_datasets/swarm_poc/val_subset.json'),\n    }\n    img_folder, ann_file = PATHS[image_set]\n    dataset = CocoDetection(img_folder, ann_file, transforms=make_coco_transforms(image_set, args.bigger), return_masks=args.masks,",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco",
        "documentation": {}
    },
    {
        "label": "CocoEvaluator",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "peekOfCode": "class CocoEvaluator(object):\n    def __init__(self, coco_gt, iou_types):\n        assert isinstance(iou_types, (list, tuple))\n        coco_gt = copy.deepcopy(coco_gt)\n        self.coco_gt = coco_gt\n        self.iou_types = iou_types\n        self.coco_eval = {}\n        for iou_type in iou_types:\n            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n        self.img_ids = []",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "documentation": {}
    },
    {
        "label": "convert_to_xywh",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "peekOfCode": "def convert_to_xywh(boxes):\n    xmin, ymin, xmax, ymax = boxes.unbind(1)\n    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\ndef merge(img_ids, eval_imgs):\n    all_img_ids = all_gather(img_ids)\n    all_eval_imgs = all_gather(eval_imgs)\n    merged_img_ids = []\n    for p in all_img_ids:\n        merged_img_ids.extend(p)\n    merged_eval_imgs = []",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "documentation": {}
    },
    {
        "label": "merge",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "peekOfCode": "def merge(img_ids, eval_imgs):\n    all_img_ids = all_gather(img_ids)\n    all_eval_imgs = all_gather(eval_imgs)\n    merged_img_ids = []\n    for p in all_img_ids:\n        merged_img_ids.extend(p)\n    merged_eval_imgs = []\n    for p in all_eval_imgs:\n        merged_eval_imgs.append(p)\n    merged_img_ids = np.array(merged_img_ids)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "documentation": {}
    },
    {
        "label": "create_common_coco_eval",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "peekOfCode": "def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n    img_ids = list(img_ids)\n    eval_imgs = list(eval_imgs.flatten())\n    coco_eval.evalImgs = eval_imgs\n    coco_eval.params.imgIds = img_ids\n    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n#################################################################\n# From pycocotools, just removed the prints and fixed\n# a Python3 bug about unicode not defined",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "peekOfCode": "def evaluate(self):\n    '''\n    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n    :return: None\n    '''\n    # tic = time.time()\n    # print('Running per image evaluation...')\n    p = self.params\n    # add backward compatibility if useSegm is specified in params\n    if p.useSegm is not None:",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_eval",
        "documentation": {}
    },
    {
        "label": "CocoPanoptic",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_panoptic",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_panoptic",
        "peekOfCode": "class CocoPanoptic:\n    def __init__(self, img_folder, ann_folder, ann_file, transforms=None, return_masks=True):\n        with open(ann_file, 'r') as f:\n            self.coco = json.load(f)\n        # sort 'images' field so that they are aligned with 'annotations'\n        # i.e., in alphabetical order\n        self.coco['images'] = sorted(self.coco['images'], key=lambda x: x['id'])\n        # sanity check\n        if \"annotations\" in self.coco:\n            for img, ann in zip(self.coco['images'], self.coco['annotations']):",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_panoptic",
        "documentation": {}
    },
    {
        "label": "build",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_panoptic",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_panoptic",
        "peekOfCode": "def build(image_set, args):\n    img_folder_root = Path(args.coco_path)\n    ann_folder_root = Path(args.coco_panoptic_path)\n    assert img_folder_root.exists(), f'provided COCO path {img_folder_root} does not exist'\n    assert ann_folder_root.exists(), f'provided COCO path {ann_folder_root} does not exist'\n    mode = 'panoptic'\n    PATHS = {\n        \"train\": (\"train2017\", Path(\"annotations\") / f'{mode}_train2017.json'),\n        \"val\": (\"val2017\", Path(\"annotations\") / f'{mode}_val2017.json'),\n    }",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.coco_panoptic",
        "documentation": {}
    },
    {
        "label": "data_prefetcher",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.data_prefetcher",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.data_prefetcher",
        "peekOfCode": "class data_prefetcher():\n    def __init__(self, loader, device, prefetch=True):\n        self.loader = iter(loader)\n        self.prefetch = prefetch\n        self.device = device\n        if prefetch:\n            self.stream = torch.cuda.Stream()\n            self.preload()\n    def preload(self):\n        try:",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.data_prefetcher",
        "documentation": {}
    },
    {
        "label": "to_cuda",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.data_prefetcher",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.data_prefetcher",
        "peekOfCode": "def to_cuda(samples, targets, device):\n    samples = samples.to(device, non_blocking=True)\n    targets = [{k: v.to(device, non_blocking=True) for k, v in t.items()} for t in targets]\n    return samples, targets\nclass data_prefetcher():\n    def __init__(self, loader, device, prefetch=True):\n        self.loader = iter(loader)\n        self.prefetch = prefetch\n        self.device = device\n        if prefetch:",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.data_prefetcher",
        "documentation": {}
    },
    {
        "label": "PanopticEvaluator",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.panoptic_eval",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.panoptic_eval",
        "peekOfCode": "class PanopticEvaluator(object):\n    def __init__(self, ann_file, ann_folder, output_dir=\"panoptic_eval\"):\n        self.gt_json = ann_file\n        self.gt_folder = ann_folder\n        if utils.is_main_process():\n            if not os.path.exists(output_dir):\n                os.mkdir(output_dir)\n        self.output_dir = output_dir\n        self.predictions = []\n    def update(self, predictions):",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.panoptic_eval",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.samplers",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.samplers",
        "peekOfCode": "class DistributedSampler(Sampler):\n    \"\"\"Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.samplers",
        "documentation": {}
    },
    {
        "label": "NodeDistributedSampler",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.samplers",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.samplers",
        "peekOfCode": "class NodeDistributedSampler(Sampler):\n    \"\"\"Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.samplers",
        "documentation": {}
    },
    {
        "label": "RandomCrop",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class RandomCrop(object):\n    def __init__(self, size):\n        self.size = size\n    def __call__(self, img, target):\n        region = T.RandomCrop.get_params(img, self.size)\n        return crop(img, target, region)\nclass RandomSizeCrop(object):\n    def __init__(self, min_size: int, max_size: int):\n        self.min_size = min_size\n        self.max_size = max_size",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "RandomSizeCrop",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class RandomSizeCrop(object):\n    def __init__(self, min_size: int, max_size: int):\n        self.min_size = min_size\n        self.max_size = max_size\n    def __call__(self, img: PIL.Image.Image, target: dict):\n        w = random.randint(self.min_size, min(img.width, self.max_size))\n        h = random.randint(self.min_size, min(img.height, self.max_size))\n        region = T.RandomCrop.get_params(img, [h, w])\n        return crop(img, target, region)\nclass CenterCrop(object):",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "CenterCrop",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class CenterCrop(object):\n    def __init__(self, size):\n        self.size = size\n    def __call__(self, img, target):\n        image_width, image_height = img.size\n        crop_height, crop_width = self.size\n        crop_top = int(round((image_height - crop_height) / 2.))\n        crop_left = int(round((image_width - crop_width) / 2.))\n        return crop(img, target, (crop_top, crop_left, crop_height, crop_width))\nclass RandomHorizontalFlip(object):",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "RandomHorizontalFlip",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class RandomHorizontalFlip(object):\n    def __init__(self, p=0.5):\n        self.p = p\n    def __call__(self, img, target):\n        if random.random() < self.p:\n            return hflip(img, target)\n        return img, target\nclass RandomResize(object):\n    def __init__(self, sizes, max_size=None):\n        assert isinstance(sizes, (list, tuple))",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "RandomResize",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class RandomResize(object):\n    def __init__(self, sizes, max_size=None):\n        assert isinstance(sizes, (list, tuple))\n        self.sizes = sizes\n        self.max_size = max_size\n    def __call__(self, img, target=None):\n        size = random.choice(self.sizes)\n        return resize(img, target, size, self.max_size)\nclass RandomPad(object):\n    def __init__(self, max_pad):",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "RandomPad",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class RandomPad(object):\n    def __init__(self, max_pad):\n        self.max_pad = max_pad\n    def __call__(self, img, target):\n        pad_x = random.randint(0, self.max_pad)\n        pad_y = random.randint(0, self.max_pad)\n        return pad(img, target, (pad_x, pad_y))\nclass RandomSelect(object):\n    \"\"\"\n    Randomly selects between transforms1 and transforms2,",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "RandomSelect",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class RandomSelect(object):\n    \"\"\"\n    Randomly selects between transforms1 and transforms2,\n    with probability p for transforms1 and (1 - p) for transforms2\n    \"\"\"\n    def __init__(self, transforms1, transforms2, p=0.5):\n        self.transforms1 = transforms1\n        self.transforms2 = transforms2\n        self.p = p\n    def __call__(self, img, target):",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "ToTensor",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class ToTensor(object):\n    def __call__(self, img, target):\n        return v2.ToTensor()(img), target\nclass RandomErasing(object):\n    def __init__(self, *args, **kwargs):\n        self.eraser = T.RandomErasing(*args, **kwargs)\n    def __call__(self, img, target):\n        return self.eraser(img), target\nclass Normalize(object):\n    def __init__(self, mean, std):",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "RandomErasing",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class RandomErasing(object):\n    def __init__(self, *args, **kwargs):\n        self.eraser = T.RandomErasing(*args, **kwargs)\n    def __call__(self, img, target):\n        return self.eraser(img), target\nclass Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n    def __call__(self, image, target=None):",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "Normalize",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n    def __call__(self, image, target=None):\n        image = F.normalize(image, mean=self.mean, std=self.std)\n        if target is None:\n            return image, None\n        target = target.copy()\n        h, w = image.shape[-2:]",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "class Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \"(\"\n        for t in self.transforms:",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "crop",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "def crop(image, target, region):\n    cropped_image = F.crop(image, *region)\n    target = target.copy()\n    i, j, h, w = region\n    # should we do something wrt the original size?\n    target[\"size\"] = torch.tensor([h, w])\n    fields = [\"labels\", \"area\", \"iscrowd\"]\n    if \"boxes\" in target:\n        boxes = target[\"boxes\"]\n        max_size = torch.as_tensor([w, h], dtype=torch.float32)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "hflip",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "def hflip(image, target):\n    flipped_image = F.hflip(image)\n    w, h = image.size\n    target = target.copy()\n    if \"boxes\" in target:\n        boxes = target[\"boxes\"]\n        boxes = boxes[:, [2, 1, 0, 3]] * torch.as_tensor([-1, 1, -1, 1]) + torch.as_tensor([w, 0, w, 0])\n        target[\"boxes\"] = boxes\n    if \"masks\" in target:\n        target['masks'] = target['masks'].flip(-1)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "resize",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "def resize(image, target, size, max_size=None):\n    # size can be min_size (scalar) or (w, h) tuple\n    def get_size_with_aspect_ratio(image_size, size, max_size=None):\n        w, h = image_size\n        if max_size is not None:\n            min_original_size = float(min((w, h)))\n            max_original_size = float(max((w, h)))\n            if max_original_size / min_original_size * size > max_size:\n                size = int(round(max_size * min_original_size / max_original_size))\n        if (w <= h and w == size) or (h <= w and h == size):",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "pad",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "description": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "peekOfCode": "def pad(image, target, padding):\n    # assumes that we only pad on the bottom right corners\n    padded_image = F.pad(image, (0, 0, padding[0], padding[1]))\n    if target is None:\n        return padded_image, None\n    target = target.copy()\n    # should we do something wrt the original size?\n    target[\"size\"] = torch.tensor(padded_image[::-1])\n    if \"masks\" in target:\n        target['masks'] = torch.nn.functional.pad(target['masks'], (0, padding[0], 0, padding[1]))",
        "detail": "swarm_poc.autobrains_poc.DETA-master.datasets_internal.transforms",
        "documentation": {}
    },
    {
        "label": "ms_deform_attn_core_pytorch",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.ops.functions.ms_deform_attn_func",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.ops.functions.ms_deform_attn_func",
        "peekOfCode": "def ms_deform_attn_core_pytorch(value, value_spatial_shapes, sampling_locations, attention_weights):\n    # for debug and test only,\n    # need to use cuda version instead\n    N_, S_, M_, D_ = value.shape\n    _, Lq_, M_, L_, P_, _ = sampling_locations.shape\n    value_list = value.split([H_ * W_ for H_, W_ in value_spatial_shapes], dim=1)\n    sampling_grids = 2 * sampling_locations - 1\n    sampling_value_list = []\n    for lid_, (H_, W_) in enumerate(value_spatial_shapes):\n        # N_, H_*W_, M_, D_ -> N_, H_*W_, M_*D_ -> N_, M_*D_, H_*W_ -> N_*M_, D_, H_, W_",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.ops.functions.ms_deform_attn_func",
        "documentation": {}
    },
    {
        "label": "MSDeformAttn",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.ops.modules.ms_deform_attn",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.ops.modules.ms_deform_attn",
        "peekOfCode": "class MSDeformAttn(nn.Module):\n    def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):\n        \"\"\"\n        Multi-Scale Deformable Attention Module\n        :param d_model      hidden dimension\n        :param n_levels     number of feature levels\n        :param n_heads      number of attention heads\n        :param n_points     number of sampling points per attention head per feature level\n        \"\"\"\n        super().__init__()",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.ops.modules.ms_deform_attn",
        "documentation": {}
    },
    {
        "label": "Matcher",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "peekOfCode": "class Matcher(object):\n    \"\"\"\n    This class assigns to each predicted \"element\" (e.g., a box) a ground-truth\n    element. Each predicted element will have exactly zero or one matches; each\n    ground-truth element may be matched to zero or more predicted elements.\n    The matching is determined by the MxN match_quality_matrix, that characterizes\n    how well each (ground-truth, prediction)-pair match each other. For example,\n    if the elements are boxes, this matrix may contain box intersection-over-union\n    overlap values.\n    The matcher returns (a) a vector of length N containing the index of the",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "documentation": {}
    },
    {
        "label": "Stage2Assigner",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "peekOfCode": "class Stage2Assigner(nn.Module):\n    def __init__(self, num_queries, max_k=4):\n        super().__init__()\n        self.positive_fraction = 0.25\n        self.bg_label = 400  # number > 91 to filter out later\n        self.batch_size_per_image = num_queries\n        self.proposal_matcher = Matcher(thresholds=[0.6], labels=[0, 1], allow_low_quality_matches=True)\n        self.k = max_k\n    def _sample_proposals(\n        self, matched_idxs: torch.Tensor, matched_labels: torch.Tensor, gt_classes: torch.Tensor",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "documentation": {}
    },
    {
        "label": "Stage1Assigner",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "peekOfCode": "class Stage1Assigner(nn.Module):\n    def __init__(self, t_low=0.3, t_high=0.7, max_k=4):\n        super().__init__()\n        self.positive_fraction = 0.5\n        self.batch_size_per_image = 256\n        self.k = max_k\n        self.t_low = t_low\n        self.t_high = t_high\n        self.anchor_matcher = Matcher(thresholds=[t_low, t_high], labels=[0, -1, 1], allow_low_quality_matches=True)\n    def _subsample_labels(self, label):",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "documentation": {}
    },
    {
        "label": "nonzero_tuple",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "peekOfCode": "def nonzero_tuple(x):\n    \"\"\"\n    A 'as_tuple=True' version of torch.nonzero to support torchscript.\n    because of https://github.com/pytorch/pytorch/issues/38718\n    \"\"\"\n    if torch.jit.is_scripting():\n        if x.dim() == 0:\n            return x.unsqueeze(0).nonzero().unbind(1)\n        return x.nonzero().unbind(1)\n    else:",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "documentation": {}
    },
    {
        "label": "subsample_labels",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "peekOfCode": "def subsample_labels(\n    labels: torch.Tensor, num_samples: int, positive_fraction: float, bg_label: int\n):\n    \"\"\"\n    Return `num_samples` (or fewer, if not enough found)\n    random samples from `labels` which is a mixture of positives & negatives.\n    It will try to return as many positives as possible without\n    exceeding `positive_fraction * num_samples`, and then try to\n    fill the remaining slots with negatives.\n    Args:",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "documentation": {}
    },
    {
        "label": "sample_topk_per_gt",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "peekOfCode": "def sample_topk_per_gt(pr_inds, gt_inds, iou, k):\n    if len(gt_inds) == 0:\n        return pr_inds, gt_inds\n    # find topk matches for each gt\n    gt_inds2, counts = gt_inds.unique(return_counts=True)\n    scores, pr_inds2 = iou[gt_inds2].topk(k, dim=1)\n    gt_inds2 = gt_inds2[:,None].repeat(1, k)\n    # filter to as many matches that gt has\n    pr_inds3 = torch.cat([pr[:c] for c, pr in zip(counts, pr_inds2)]).to(pr_inds2.device)\n    gt_inds3 = torch.cat([gt[:c] for c, gt in zip(counts, gt_inds2)]).to(pr_inds2.device)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.assigner",
        "documentation": {}
    },
    {
        "label": "FrozenBatchNorm2d",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "peekOfCode": "class FrozenBatchNorm2d(torch.nn.Module):\n    \"\"\"\n    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n    without which any other models than torchvision.models.resnet[18,34,50,101]\n    produce nans.\n    \"\"\"\n    def __init__(self, n, eps=1e-5):\n        super(FrozenBatchNorm2d, self).__init__()\n        self.register_buffer(\"weight\", torch.ones(n))",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "documentation": {}
    },
    {
        "label": "BackboneBase",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "peekOfCode": "class BackboneBase(nn.Module):\n    def __init__(self, backbone: nn.Module, train_backbone: bool, return_interm_layers: bool):\n        super().__init__()\n        for name, parameter in backbone.named_parameters():\n            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n                parameter.requires_grad_(False)\n        if return_interm_layers:\n            # return_layers = {\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"}\n            return_layers = {\"layer2\": \"0\", \"layer3\": \"1\", \"layer4\": \"2\"}\n            self.strides = [8, 16, 32]",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "documentation": {}
    },
    {
        "label": "Backbone",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "peekOfCode": "class Backbone(BackboneBase):\n    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n    def __init__(self, name: str,\n                 train_backbone: bool,\n                 return_interm_layers: bool,\n                 dilation: bool):\n        norm_layer = FrozenBatchNorm2d\n        backbone = getattr(torchvision.models, name)(\n            replace_stride_with_dilation=[False, False, dilation],\n            pretrained=is_main_process(), norm_layer=norm_layer)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "documentation": {}
    },
    {
        "label": "SwinBackbone",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "peekOfCode": "class SwinBackbone(nn.Module):\n    def __init__(self):\n        # we skip R50 FrozenBatchNorm2d, dilation, train l{2,3,4} only\n        super().__init__()\n        self.body = get_swinl()\n        self.features = ['res3', 'res4', 'res5']\n        self.strides = [8, 16, 32]\n        self.num_channels = [384, 768, 1536]\n    def forward(self, tensor_list: NestedTensor):\n        xs = self.body(tensor_list.tensors)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "documentation": {}
    },
    {
        "label": "Joiner",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "peekOfCode": "class Joiner(nn.Sequential):\n    def __init__(self, backbone, position_embedding):\n        super().__init__(backbone, position_embedding)\n        self.strides = backbone.strides\n        self.num_channels = backbone.num_channels\n    def forward(self, tensor_list: NestedTensor):\n        xs = self[0](tensor_list)\n        out: List[NestedTensor] = []\n        pos = []\n        for name, x in sorted(xs.items()):",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "documentation": {}
    },
    {
        "label": "build_backbone",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "peekOfCode": "def build_backbone(args):\n    position_embedding = build_position_encoding(args)\n    train_backbone = args.lr_backbone > 0\n    return_interm_layers = args.masks or (args.num_feature_levels > 1)\n    if 'swin' in args.backbone:\n        backbone = SwinBackbone()\n    else:\n        backbone = Backbone(args.backbone, train_backbone, return_interm_layers, args.dilation)\n    model = Joiner(backbone, position_embedding)\n    return model",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.backbone",
        "documentation": {}
    },
    {
        "label": "DeformableDETR",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "peekOfCode": "class DeformableDETR(nn.Module):\n    \"\"\" This is the Deformable DETR module that performs object detection \"\"\"\n    def __init__(self, backbone, transformer, num_classes, num_queries, num_feature_levels,\n                 aux_loss=True, with_box_refine=False, two_stage=False):\n        \"\"\" Initializes the model.\n        Parameters:\n            backbone: torch module of the backbone to be used. See backbone.py\n            transformer: torch module of the transformer architecture. See transformer.py\n            num_classes: number of object classes\n            num_queries: number of object queries, ie detection slot. This is the maximal number of objects",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "documentation": {}
    },
    {
        "label": "SetCriterion",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "peekOfCode": "class SetCriterion(nn.Module):\n    \"\"\" This class computes the loss for DETR.\n    The process happens in two steps:\n        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n    \"\"\"\n    def __init__(self, num_classes, matcher, weight_dict, losses, focal_alpha=0.25,\n                 num_queries=300, assign_first_stage=False, assign_second_stage=False):\n        \"\"\" Create the criterion.\n        Parameters:",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "documentation": {}
    },
    {
        "label": "PostProcess",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "peekOfCode": "class PostProcess(nn.Module):\n    \"\"\" This module converts the model's output into the format expected by the coco api\"\"\"\n    @torch.no_grad()\n    def forward(self, outputs, target_sizes):\n        \"\"\" Perform the computation\n        Parameters:\n            outputs: raw outputs of the model\n            target_sizes: tensor of dimension [batch_size x 2] containing the size of each images of the batch\n                          For evaluation, this must be the original image size (before any data augmentation)\n                          For visualization, this should be the image size after data augment, but before padding",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "documentation": {}
    },
    {
        "label": "NMSPostProcess",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "peekOfCode": "class NMSPostProcess(nn.Module):\n    \"\"\" This module converts the model's output into the format expected by the coco api\"\"\"\n    @torch.no_grad()\n    def forward(self, outputs, target_sizes):\n        \"\"\" Perform the computation\n        Parameters:\n            outputs: raw outputs of the model\n            target_sizes: tensor of dimension [batch_size x 2] containing the size of each images of the batch\n                          For evaluation, this must be the original image size (before any data augmentation)\n                          For visualization, this should be the image size after data augment, but before padding",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "peekOfCode": "class MLP(nn.Module):\n    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n        super().__init__()\n        self.num_layers = num_layers\n        h = [hidden_dim] * (num_layers - 1)\n        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n    def forward(self, x):\n        for i, layer in enumerate(self.layers):\n            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "documentation": {}
    },
    {
        "label": "build",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "peekOfCode": "def build(args):\n    num_classes = 20 if args.dataset_file != 'coco' else 91\n    if args.dataset_file == \"coco_panoptic\":\n        num_classes = 250\n    device = torch.device(args.device)\n    backbone = build_backbone(args)\n    transformer = build_deforamble_transformer(args)\n    model = DeformableDETR(\n        backbone,\n        transformer,",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.deformable_detr",
        "documentation": {}
    },
    {
        "label": "DeformableTransformer",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "peekOfCode": "class DeformableTransformer(nn.Module):\n    def __init__(self, d_model=256, nhead=8,\n                 num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=1024, dropout=0.1,\n                 activation=\"relu\", return_intermediate_dec=False,\n                 num_feature_levels=4, dec_n_points=4,  enc_n_points=4,\n                 two_stage=False, two_stage_num_proposals=300,\n                 assign_first_stage=False):\n        super().__init__()\n        self.d_model = d_model\n        self.nhead = nhead",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "documentation": {}
    },
    {
        "label": "DeformableTransformerEncoderLayer",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "peekOfCode": "class DeformableTransformerEncoderLayer(nn.Module):\n    def __init__(self,\n                 d_model=256, d_ffn=1024,\n                 dropout=0.1, activation=\"relu\",\n                 n_levels=4, n_heads=8, n_points=4):\n        super().__init__()\n        # self attention\n        self.self_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)\n        self.dropout1 = nn.Dropout(dropout)\n        self.norm1 = nn.LayerNorm(d_model)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "documentation": {}
    },
    {
        "label": "DeformableTransformerEncoder",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "peekOfCode": "class DeformableTransformerEncoder(nn.Module):\n    def __init__(self, encoder_layer, num_layers):\n        super().__init__()\n        self.layers = _get_clones(encoder_layer, num_layers)\n        self.num_layers = num_layers\n    @staticmethod\n    def get_reference_points(spatial_shapes, valid_ratios, device):\n        reference_points_list = []\n        for lvl, (H_, W_) in enumerate(spatial_shapes):\n            ref_y, ref_x = torch.meshgrid(torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32, device=device),",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "documentation": {}
    },
    {
        "label": "DeformableTransformerDecoderLayer",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "peekOfCode": "class DeformableTransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model=256, d_ffn=1024,\n                 dropout=0.1, activation=\"relu\",\n                 n_levels=4, n_heads=8, n_points=4):\n        super().__init__()\n        # cross attention\n        self.cross_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)\n        self.dropout1 = nn.Dropout(dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        # self attention",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "documentation": {}
    },
    {
        "label": "DeformableTransformerDecoder",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "peekOfCode": "class DeformableTransformerDecoder(nn.Module):\n    def __init__(self, decoder_layer, num_layers, return_intermediate=False):\n        super().__init__()\n        self.layers = _get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.return_intermediate = return_intermediate\n        # hack implementation for iterative bounding box refinement and two-stage Deformable DETR\n        self.bbox_embed = None\n        self.class_embed = None\n    def forward(self, tgt, reference_points, src, src_spatial_shapes, src_level_start_index, src_valid_ratios,",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "documentation": {}
    },
    {
        "label": "build_deforamble_transformer",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "peekOfCode": "def build_deforamble_transformer(args):\n    return DeformableTransformer(\n        d_model=args.hidden_dim,\n        nhead=args.nheads,\n        num_encoder_layers=args.enc_layers,\n        num_decoder_layers=args.dec_layers,\n        dim_feedforward=args.dim_feedforward,\n        dropout=args.dropout,\n        activation=\"relu\",\n        return_intermediate_dec=True,",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.deformable_transformer",
        "documentation": {}
    },
    {
        "label": "HungarianMatcher",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.matcher",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.matcher",
        "peekOfCode": "class HungarianMatcher(nn.Module):\n    \"\"\"This class computes an assignment between the targets and the predictions of the network\n    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n    while the others are un-matched (and thus treated as non-objects).\n    \"\"\"\n    def __init__(self,\n                 cost_class: float = 1,\n                 cost_bbox: float = 1,\n                 cost_giou: float = 1):",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.matcher",
        "documentation": {}
    },
    {
        "label": "build_matcher",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.matcher",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.matcher",
        "peekOfCode": "def build_matcher(args):\n    return HungarianMatcher(cost_class=args.set_cost_class,\n                            cost_bbox=args.set_cost_bbox,\n                            cost_giou=args.set_cost_giou)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.matcher",
        "documentation": {}
    },
    {
        "label": "PositionEmbeddingSine",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.position_encoding",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.position_encoding",
        "peekOfCode": "class PositionEmbeddingSine(nn.Module):\n    \"\"\"\n    This is a more standard version of the position embedding, very similar to the one\n    used by the Attention is all you need paper, generalized to work on images.\n    \"\"\"\n    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n        super().__init__()\n        self.num_pos_feats = num_pos_feats\n        self.temperature = temperature\n        self.normalize = normalize",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.position_encoding",
        "documentation": {}
    },
    {
        "label": "PositionEmbeddingLearned",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.position_encoding",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.position_encoding",
        "peekOfCode": "class PositionEmbeddingLearned(nn.Module):\n    \"\"\"\n    Absolute pos embedding, learned.\n    \"\"\"\n    def __init__(self, num_pos_feats=256):\n        super().__init__()\n        self.row_embed = nn.Embedding(50, num_pos_feats)\n        self.col_embed = nn.Embedding(50, num_pos_feats)\n        self.reset_parameters()\n    def reset_parameters(self):",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.position_encoding",
        "documentation": {}
    },
    {
        "label": "build_position_encoding",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.position_encoding",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.position_encoding",
        "peekOfCode": "def build_position_encoding(args):\n    N_steps = args.hidden_dim // 2\n    if args.position_embedding in ('v2', 'sine'):\n        # TODO find a better way of exposing other arguments\n        position_embedding = PositionEmbeddingSine(N_steps, normalize=True)\n    elif args.position_embedding in ('v3', 'learned'):\n        position_embedding = PositionEmbeddingLearned(N_steps)\n    else:\n        raise ValueError(f\"not supported {args.position_embedding}\")\n    return position_embedding",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.position_encoding",
        "documentation": {}
    },
    {
        "label": "DETRsegm",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "peekOfCode": "class DETRsegm(nn.Module):\n    def __init__(self, detr, freeze_detr=False):\n        super().__init__()\n        self.detr = detr\n        if freeze_detr:\n            for p in self.parameters():\n                p.requires_grad_(False)\n        hidden_dim, nheads = detr.transformer.d_model, detr.transformer.nhead\n        self.bbox_attention = MHAttentionMap(hidden_dim, hidden_dim, nheads, dropout=0)\n        self.mask_head = MaskHeadSmallConv(hidden_dim + nheads, [1024, 512, 256], hidden_dim)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "documentation": {}
    },
    {
        "label": "MaskHeadSmallConv",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "peekOfCode": "class MaskHeadSmallConv(nn.Module):\n    \"\"\"\n    Simple convolutional head, using group norm.\n    Upsampling is done using a FPN approach\n    \"\"\"\n    def __init__(self, dim, fpn_dims, context_dim):\n        super().__init__()\n        inter_dims = [dim, context_dim // 2, context_dim // 4, context_dim // 8, context_dim // 16, context_dim // 64]\n        self.lay1 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n        self.gn1 = torch.nn.GroupNorm(8, dim)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "documentation": {}
    },
    {
        "label": "MHAttentionMap",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "peekOfCode": "class MHAttentionMap(nn.Module):\n    \"\"\"This is a 2D attention module, which only returns the attention softmax (no multiplication by value)\"\"\"\n    def __init__(self, query_dim, hidden_dim, num_heads, dropout=0, bias=True):\n        super().__init__()\n        self.num_heads = num_heads\n        self.hidden_dim = hidden_dim\n        self.dropout = nn.Dropout(dropout)\n        self.q_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n        self.k_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n        nn.init.zeros_(self.k_linear.bias)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "documentation": {}
    },
    {
        "label": "PostProcessSegm",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "peekOfCode": "class PostProcessSegm(nn.Module):\n    def __init__(self, threshold=0.5):\n        super().__init__()\n        self.threshold = threshold\n    @torch.no_grad()\n    def forward(self, results, outputs, orig_target_sizes, max_target_sizes):\n        assert len(orig_target_sizes) == len(max_target_sizes)\n        max_h, max_w = max_target_sizes.max(0)[0].tolist()\n        outputs_masks = outputs[\"pred_masks\"].squeeze(2)\n        outputs_masks = F.interpolate(outputs_masks, size=(max_h, max_w), mode=\"bilinear\", align_corners=False)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "documentation": {}
    },
    {
        "label": "PostProcessPanoptic",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "peekOfCode": "class PostProcessPanoptic(nn.Module):\n    \"\"\"This class converts the output of the model to the final panoptic result, in the format expected by the\n    coco panoptic API \"\"\"\n    def __init__(self, is_thing_map, threshold=0.85):\n        \"\"\"\n        Parameters:\n           is_thing_map: This is a whose keys are the class ids, and the values a boolean indicating whether\n                          the class is  a thing (True) or a stuff (False) class\n           threshold: confidence threshold: segments with confidence lower than this will be deleted\n        \"\"\"",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "documentation": {}
    },
    {
        "label": "dice_loss",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "peekOfCode": "def dice_loss(inputs, targets, num_boxes):\n    \"\"\"\n    Compute the DICE loss, similar to generalized IOU for masks\n    Args:\n        inputs: A float tensor of arbitrary shape.\n                The predictions for each example.\n        targets: A float tensor with the same shape as inputs. Stores the binary\n                 classification label for each element in inputs\n                (0 for the negative class and 1 for the positive class).\n    \"\"\"",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "documentation": {}
    },
    {
        "label": "sigmoid_focal_loss",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "peekOfCode": "def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n    \"\"\"\n    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n    Args:\n        inputs: A float tensor of arbitrary shape.\n                The predictions for each example.\n        targets: A float tensor with the same shape as inputs. Stores the binary\n                 classification label for each element in inputs\n                (0 for the negative class and 1 for the positive class).\n        alpha: (optional) Weighting factor in range (0,1) to balance",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.segmentation",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "class Mlp(nn.Module):\n    \"\"\"Multilayer perceptron.\"\"\"\n    def __init__(\n        self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.0\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "WindowAttention",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "class WindowAttention(nn.Module):\n    \"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "SwinTransformerBlock",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "class SwinTransformerBlock(nn.Module):\n    \"\"\"Swin Transformer Block.\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "PatchMerging",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "class PatchMerging(nn.Module):\n    \"\"\"Patch Merging Layer\n    Args:\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n    def __init__(self, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "BasicLayer",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "class BasicLayer(nn.Module):\n    \"\"\"A basic Swin Transformer layer for one stage.\n    Args:\n        dim (int): Number of feature channels\n        depth (int): Depths of this stage.\n        num_heads (int): Number of attention head.\n        window_size (int): Local window size. Default: 7.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\n    Args:\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "SwinTransformer",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "class SwinTransformer(nn.Module):\n    \"\"\"Swin Transformer backbone.\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n    Args:\n        pretrain_img_size (int): Input image size for training the pretrained model,\n            used in absolute postion embedding. Default 224.\n        patch_size (int | tuple(int)): Patch size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "get_swinl",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "def get_swinl(**add_kwargs):\n    model = SwinTransformer(**swin_l_kwargs, **add_kwargs)\n    state_dict = torch.load(swin_l_weights, map_location=torch.device('cpu'))\n    load_info = model.load_state_dict(state_dict['model'], strict=False,)\n    print('Missing swin keys', load_info.missing_keys)\n    print('Unexpected swin keys', load_info.unexpected_keys)\n    return model\nclass Mlp(nn.Module):\n    \"\"\"Multilayer perceptron.\"\"\"\n    def __init__(",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "window_partition",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "def window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "window_reverse",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "def window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "swin_l_weights",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "description": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "peekOfCode": "swin_l_weights = './weights/converted_deta_swin_o365_finetune.pth'\ndef get_swinl(**add_kwargs):\n    model = SwinTransformer(**swin_l_kwargs, **add_kwargs)\n    state_dict = torch.load(swin_l_weights, map_location=torch.device('cpu'))\n    load_info = model.load_state_dict(state_dict['model'], strict=False,)\n    print('Missing swin keys', load_info.missing_keys)\n    print('Unexpected swin keys', load_info.unexpected_keys)\n    return model\nclass Mlp(nn.Module):\n    \"\"\"Multilayer perceptron.\"\"\"",
        "detail": "swarm_poc.autobrains_poc.DETA-master.models.swin",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.tools.launch",
        "description": "swarm_poc.autobrains_poc.DETA-master.tools.launch",
        "peekOfCode": "def parse_args():\n    \"\"\"\n    Helper function parsing the command line options\n    @retval ArgumentParser\n    \"\"\"\n    parser = ArgumentParser(description=\"PyTorch distributed training launch \"\n                                        \"helper utilty that will spawn up \"\n                                        \"multiple distributed processes\")\n    # Optional arguments for the launch helper\n    parser.add_argument(\"--nnodes\", type=int, default=1,",
        "detail": "swarm_poc.autobrains_poc.DETA-master.tools.launch",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.tools.launch",
        "description": "swarm_poc.autobrains_poc.DETA-master.tools.launch",
        "peekOfCode": "def main():\n    args = parse_args()\n    # world size in terms of number of processes\n    dist_world_size = args.nproc_per_node * args.nnodes\n    # set PyTorch distributed related environmental variables\n    current_env = os.environ.copy()\n    current_env[\"MASTER_ADDR\"] = args.master_addr\n    current_env[\"MASTER_PORT\"] = str(args.master_port)\n    current_env[\"WORLD_SIZE\"] = str(dist_world_size)\n    processes = []",
        "detail": "swarm_poc.autobrains_poc.DETA-master.tools.launch",
        "documentation": {}
    },
    {
        "label": "box_cxcywh_to_xyxy",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "peekOfCode": "def box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(-1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=-1)\ndef box_xyxy_to_cxcywh(x):\n    x0, y0, x1, y1 = x.unbind(-1)\n    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n         (x1 - x0), (y1 - y0)]\n    return torch.stack(b, dim=-1)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_xyxy_to_cxcywh",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "peekOfCode": "def box_xyxy_to_cxcywh(x):\n    x0, y0, x1, y1 = x.unbind(-1)\n    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n         (x1 - x0), (y1 - y0)]\n    return torch.stack(b, dim=-1)\n# modified from torchvision to also return the union\ndef box_iou(boxes1, boxes2):\n    area1 = box_area(boxes1)\n    area2 = box_area(boxes2)\n    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_iou",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "peekOfCode": "def box_iou(boxes1, boxes2):\n    area1 = box_area(boxes1)\n    area2 = box_area(boxes2)\n    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n    union = area1[:, None] + area2 - inter\n    iou = inter / union\n    return iou, union",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "documentation": {}
    },
    {
        "label": "generalized_box_iou",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "peekOfCode": "def generalized_box_iou(boxes1, boxes2):\n    \"\"\"\n    Generalized IoU from https://giou.stanford.edu/\n    The boxes should be in [x0, y0, x1, y1] format\n    Returns a [N, M] pairwise matrix, where N = len(boxes1)\n    and M = len(boxes2)\n    \"\"\"\n    # degenerate boxes gives inf / nan results\n    # so do an early check\n    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "documentation": {}
    },
    {
        "label": "masks_to_boxes",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "peekOfCode": "def masks_to_boxes(masks):\n    \"\"\"Compute the bounding boxes around the provided masks\n    The masks should be in format [N, H, W] where N is the number of masks, (H, W) are the spatial dimensions.\n    Returns a [N, 4] tensors, with the boxes in xyxy format\n    \"\"\"\n    if masks.numel() == 0:\n        return torch.zeros((0, 4), device=masks.device)\n    h, w = masks.shape[-2:]\n    y = torch.arange(0, h, dtype=torch.float)\n    x = torch.arange(0, w, dtype=torch.float)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.box_ops",
        "documentation": {}
    },
    {
        "label": "SmoothedValue",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "class SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "MetricLogger",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "class MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\"):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "class NestedTensor(object):\n    def __init__(self, tensors, mask: Optional[Tensor]):\n        self.tensors = tensors\n        self.mask = mask\n    def to(self, device, non_blocking=False):\n        # type: (Device) -> NestedTensor # noqa\n        cast_tensor = self.tensors.to(device, non_blocking=non_blocking)\n        mask = self.mask\n        if mask is not None:\n            assert mask is not None",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "all_gather",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def all_gather(data):\n    \"\"\"\n    Run all_gather on arbitrary picklable data (not necessarily tensors)\n    Args:\n        data: any picklable object\n    Returns:\n        list[data]: list of data gathered from each rank\n    \"\"\"\n    world_size = get_world_size()\n    if world_size == 1:",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "reduce_dict",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def reduce_dict(input_dict, average=True):\n    \"\"\"\n    Args:\n        input_dict (dict): all the values will be reduced\n        average (bool): whether to do average or sum\n    Reduce the values in the dictionary from all processes so that all processes\n    have the averaged results. Returns a dict with the same fields as\n    input_dict, after reduction.\n    \"\"\"\n    world_size = get_world_size()",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "get_sha",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def get_sha():\n    cwd = os.path.dirname(os.path.abspath(__file__))\n    def _run(command):\n        return subprocess.check_output(command, cwd=cwd).decode('ascii').strip()\n    sha = 'N/A'\n    diff = \"clean\"\n    branch = 'N/A'\n    try:\n        sha = _run(['git', 'rev-parse', 'HEAD'])\n        subprocess.check_output(['git', 'diff'], cwd=cwd)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "collate_fn",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def collate_fn(batch):\n    batch = list(zip(*batch))\n    return tuple(batch)\ndef _max_by_axis(the_list):\n    # type: (List[List[int]]) -> List[int]\n    maxes = the_list[0]\n    for sublist in the_list[1:]:\n        for index, item in enumerate(sublist):\n            maxes[index] = max(maxes[index], item)\n    return maxes",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "nested_tensor_from_tensor_list",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n    # TODO make this more general\n    if tensor_list[0].ndim == 3:\n        # TODO make it support different-sized images\n        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n        batch_shape = [len(tensor_list)] + max_size\n        b, c, h, w = batch_shape\n        dtype = tensor_list[0].dtype\n        device = tensor_list[0].device",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "setup_for_distributed",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def setup_for_distributed(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n    def print(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "is_dist_avail_and_initialized",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "get_world_size",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()\ndef get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\ndef get_local_size():\n    if not is_dist_avail_and_initialized():",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "get_rank",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\ndef get_local_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return int(os.environ['LOCAL_SIZE'])\ndef get_local_rank():\n    if not is_dist_avail_and_initialized():",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "get_local_size",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def get_local_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return int(os.environ['LOCAL_SIZE'])\ndef get_local_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return int(os.environ['LOCAL_RANK'])\ndef is_main_process():\n    return get_rank() == 0",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "get_local_rank",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def get_local_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return int(os.environ['LOCAL_RANK'])\ndef is_main_process():\n    return get_rank() == 0\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\ndef init_distributed_mode(args):",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "is_main_process",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def is_main_process():\n    return get_rank() == 0\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\ndef init_distributed_mode(args):\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "save_on_master",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\ndef init_distributed_mode(args):\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])\n        args.dist_url = 'env://'\n        os.environ['LOCAL_SIZE'] = str(torch.cuda.device_count())",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "init_distributed_mode",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def init_distributed_mode(args):\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])\n        args.dist_url = 'env://'\n        os.environ['LOCAL_SIZE'] = str(torch.cuda.device_count())\n    elif 'SLURM_PROCID' in os.environ:\n        proc_id = int(os.environ['SLURM_PROCID'])\n        ntasks = int(os.environ['SLURM_NTASKS'])",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n    if target.numel() == 0:\n        return [torch.zeros([], device=output.device)]\n    maxk = max(topk)\n    batch_size = target.size(0)\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n    res = []",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def interpolate(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n    # type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool]) -> Tensor\n    \"\"\"\n    Equivalent to nn.functional.interpolate, but with support for empty batch sizes.\n    This will eventually be supported natively by PyTorch, and this\n    class can go away.\n    \"\"\"\n    if float(torchvision.__version__[:3]) < 0.7:\n        if input.numel() > 0:\n            return torch.nn.functional.interpolate(",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "get_total_grad_norm",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def get_total_grad_norm(parameters, norm_type=2):\n    parameters = list(filter(lambda p: p.grad is not None, parameters))\n    norm_type = float(norm_type)\n    device = parameters[0].grad.device\n    total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]),\n                            norm_type)\n    return total_norm\ndef inverse_sigmoid(x, eps=1e-5):\n    x = x.clamp(min=0, max=1)\n    x1 = x.clamp(min=eps)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "inverse_sigmoid",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "peekOfCode": "def inverse_sigmoid(x, eps=1e-5):\n    x = x.clamp(min=0, max=1)\n    x1 = x.clamp(min=eps)\n    x2 = (1 - x).clamp(min=eps)\n    return torch.log(x1 / x2)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.misc",
        "documentation": {}
    },
    {
        "label": "plot_logs",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.plot_utils",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.plot_utils",
        "peekOfCode": "def plot_logs(logs, fields=('class_error', 'loss_bbox_unscaled', 'mAP'), ewm_col=0, log_name='log.txt'):\n    '''\n    Function to plot specific fields from training log(s). Plots both training and test results.\n    :: Inputs - logs = list containing Path objects, each pointing to individual dir with a log file\n              - fields = which results to plot from each log file - plots both training and test for each field.\n              - ewm_col = optional, which column to use as the exponential weighted smoothing of the plots\n              - log_name = optional, name of log file if different than default 'log.txt'.\n    :: Outputs - matplotlib plots of results in fields, color coded for each log file.\n               - solid lines are training results, dashed lines are test results.\n    '''",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.plot_utils",
        "documentation": {}
    },
    {
        "label": "plot_precision_recall",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.util.plot_utils",
        "description": "swarm_poc.autobrains_poc.DETA-master.util.plot_utils",
        "peekOfCode": "def plot_precision_recall(files, naming_scheme='iter'):\n    if naming_scheme == 'exp_id':\n        # name becomes exp_id\n        names = [f.parts[-3] for f in files]\n    elif naming_scheme == 'iter':\n        names = [f.stem for f in files]\n    else:\n        raise ValueError(f'not supported {naming_scheme}')\n    fig, axs = plt.subplots(ncols=2, figsize=(16, 5))\n    for f, color, name in zip(files, sns.color_palette(\"Blues\", n_colors=len(files)), names):",
        "detail": "swarm_poc.autobrains_poc.DETA-master.util.plot_utils",
        "documentation": {}
    },
    {
        "label": "get_benckmark_arg_parser",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.benchmark",
        "description": "swarm_poc.autobrains_poc.DETA-master.benchmark",
        "peekOfCode": "def get_benckmark_arg_parser():\n    parser = argparse.ArgumentParser('Benchmark inference speed of Deformable DETR.')\n    parser.add_argument('--num_iters', type=int, default=300, help='total iters to benchmark speed')\n    parser.add_argument('--warm_iters', type=int, default=5, help='ignore first several iters that are very slow')\n    parser.add_argument('--batch_size', type=int, default=1, help='batch size in inference')\n    parser.add_argument('--resume', type=str, help='load the pre-trained checkpoint')\n    return parser\n@torch.no_grad()\ndef measure_average_inference_time(model, inputs, num_iters=100, warm_iters=5):\n    ts = []",
        "detail": "swarm_poc.autobrains_poc.DETA-master.benchmark",
        "documentation": {}
    },
    {
        "label": "measure_average_inference_time",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.benchmark",
        "description": "swarm_poc.autobrains_poc.DETA-master.benchmark",
        "peekOfCode": "def measure_average_inference_time(model, inputs, num_iters=100, warm_iters=5):\n    ts = []\n    for iter_ in range(num_iters):\n        torch.cuda.synchronize()\n        t_ = time.perf_counter()\n        model(inputs)\n        torch.cuda.synchronize()\n        t = time.perf_counter() - t_\n        if iter_ >= warm_iters:\n          ts.append(t)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.benchmark",
        "documentation": {}
    },
    {
        "label": "benchmark",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.benchmark",
        "description": "swarm_poc.autobrains_poc.DETA-master.benchmark",
        "peekOfCode": "def benchmark():\n    args, _ = get_benckmark_arg_parser().parse_known_args()\n    main_args = get_main_args_parser().parse_args(_)\n    assert args.warm_iters < args.num_iters and args.num_iters > 0 and args.warm_iters >= 0\n    assert args.batch_size > 0\n    assert args.resume is None or os.path.exists(args.resume)\n    dataset = build_dataset('val', main_args)\n    model, _, _ = build_model(main_args)\n    model.cuda()\n    model.eval()",
        "detail": "swarm_poc.autobrains_poc.DETA-master.benchmark",
        "documentation": {}
    },
    {
        "label": "train_one_epoch",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.engine",
        "description": "swarm_poc.autobrains_poc.DETA-master.engine",
        "peekOfCode": "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n                    device: torch.device, epoch: int, max_norm: float = 0):\n    model.train()\n    criterion.train()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n    metric_logger.add_meter('grad_norm', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n    header = 'Epoch: [{}]'.format(epoch)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.engine",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.engine",
        "description": "swarm_poc.autobrains_poc.DETA-master.engine",
        "peekOfCode": "def evaluate(model, criterion, postprocessors, data_loader, base_ds, device, output_dir):\n    model.eval()\n    criterion.eval()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n    header = 'Test:'\n    iou_types = tuple(k for k in ('segm', 'bbox') if k in postprocessors.keys())\n    coco_evaluator = CocoEvaluator(base_ds, iou_types)\n    # coco_evaluator.coco_eval[iou_types[0]].params.iouThrs = [0, 0.1, 0.5, 0.75]\n    panoptic_evaluator = None",
        "detail": "swarm_poc.autobrains_poc.DETA-master.engine",
        "documentation": {}
    },
    {
        "label": "DETAModel",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "class DETAModel(L.LightningModule):\n    def __init__(self, model, criterion, postprocessors, args):\n        super().__init__()\n        self.model = model\n        self.criterion = criterion\n        self.postprocessors = postprocessors\n        self.args = args\n    def configure_optimizers(self):\n        param_dicts = [\n            {",
        "detail": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "def get_args_parser():\n    parser = argparse.ArgumentParser('Deformable DETR Detector', add_help=False)\n    parser.add_argument('--lr', default=5e-5, type=float)\n    parser.add_argument('--lr_backbone_names', default=[\"backbone.0\"], type=str, nargs='+')\n    parser.add_argument('--lr_backbone', default=5e-6, type=float)\n    parser.add_argument('--lr_linear_proj_names', default=['reference_points', 'sampling_offsets'], type=str, nargs='+')\n    parser.add_argument('--lr_linear_proj_mult', default=0.1, type=float)\n    parser.add_argument('--batch_size', default=2, type=int)\n    parser.add_argument('--weight_decay', default=1e-4, type=float)\n    parser.add_argument('--epochs', default=24, type=int)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "match_name_keywords",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "def match_name_keywords(n, name_keywords):\n    out = False\n    for b in name_keywords:\n        if b in n:\n            out = True\n            break\n    return out\nif args.dataset_file == \"coco_panoptic\":\n    # We also evaluate AP during panoptic training, on original coco DS\n    coco_val = datasets.coco.build(\"val\", args)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "swarm_one_client",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "swarm_one_client = Client(api_key=\"bSNxj8I21w\")\ndef get_args_parser():\n    parser = argparse.ArgumentParser('Deformable DETR Detector', add_help=False)\n    parser.add_argument('--lr', default=5e-5, type=float)\n    parser.add_argument('--lr_backbone_names', default=[\"backbone.0\"], type=str, nargs='+')\n    parser.add_argument('--lr_backbone', default=5e-6, type=float)\n    parser.add_argument('--lr_linear_proj_names', default=['reference_points', 'sampling_offsets'], type=str, nargs='+')\n    parser.add_argument('--lr_linear_proj_mult', default=0.1, type=float)\n    parser.add_argument('--batch_size', default=2, type=int)\n    parser.add_argument('--weight_decay', default=1e-4, type=float)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "parser = argparse.ArgumentParser('Deformable DETR training and evaluation script', parents=[get_args_parser()])\ncustom_args = '--with_box_refine --two_stage \\\n    --num_feature_levels 5 --num_queries 900 \\\n    --dim_feedforward 2048 --dropout 0.0 --cls_loss_coef 1.0 \\\n    --assign_first_stage --assign_second_stage \\\n    --epochs 24 --lr_drop 20 \\\n    --lr 5e-5 --lr_backbone 5e-6 --batch_size 1 \\\n    --backbone swin \\\n    --bigger'.split()\nargs = parser.parse_args(custom_args)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "custom_args",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "custom_args = '--with_box_refine --two_stage \\\n    --num_feature_levels 5 --num_queries 900 \\\n    --dim_feedforward 2048 --dropout 0.0 --cls_loss_coef 1.0 \\\n    --assign_first_stage --assign_second_stage \\\n    --epochs 24 --lr_drop 20 \\\n    --lr 5e-5 --lr_backbone 5e-6 --batch_size 1 \\\n    --backbone swin \\\n    --bigger'.split()\nargs = parser.parse_args(custom_args)\nmodel, criterion, postprocessors = build_model(args)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "args = parser.parse_args(custom_args)\nmodel, criterion, postprocessors = build_model(args)\ndataset_train = build_dataset(image_set='train', args=args)\ndataset_val = build_dataset(image_set='val', args=args)\ndata_loader_train = DataLoader(dataset_train, 2, shuffle=True,\n                               collate_fn=utils.collate_fn, num_workers=0,\n                               pin_memory=True)\ndata_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "dataset_train",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "dataset_train = build_dataset(image_set='train', args=args)\ndataset_val = build_dataset(image_set='val', args=args)\ndata_loader_train = DataLoader(dataset_train, 2, shuffle=True,\n                               collate_fn=utils.collate_fn, num_workers=0,\n                               pin_memory=True)\ndata_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)\ndef match_name_keywords(n, name_keywords):\n    out = False",
        "detail": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "dataset_val",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "dataset_val = build_dataset(image_set='val', args=args)\ndata_loader_train = DataLoader(dataset_train, 2, shuffle=True,\n                               collate_fn=utils.collate_fn, num_workers=0,\n                               pin_memory=True)\ndata_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)\ndef match_name_keywords(n, name_keywords):\n    out = False\n    for b in name_keywords:",
        "detail": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "data_loader_train",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "data_loader_train = DataLoader(dataset_train, 2, shuffle=True,\n                               collate_fn=utils.collate_fn, num_workers=0,\n                               pin_memory=True)\ndata_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)\ndef match_name_keywords(n, name_keywords):\n    out = False\n    for b in name_keywords:\n        if b in n:",
        "detail": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "data_loader_val",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "data_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)\ndef match_name_keywords(n, name_keywords):\n    out = False\n    for b in name_keywords:\n        if b in n:\n            out = True\n            break\n    return out",
        "detail": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "model = DETAModel(model=model,\n                  criterion=criterion,\n                  postprocessors=postprocessors,\n                  args=args)\nhyperparameters = {\n    \"max_epochs\": [5],\n    \"batch_sizes\": [16],\n}\njob_id = swarm_one_client.fit(\n    model=model,",
        "detail": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "hyperparameters",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "hyperparameters = {\n    \"max_epochs\": [5],\n    \"batch_sizes\": [16],\n}\njob_id = swarm_one_client.fit(\n    model=model,\n    train_dataloaders=data_loader_train,\n    val_dataloaders=data_loader_val,\n    hyperparameters=hyperparameters\n)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "job_id",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "job_id = swarm_one_client.fit(\n    model=model,\n    train_dataloaders=data_loader_train,\n    val_dataloaders=data_loader_val,\n    hyperparameters=hyperparameters\n)\nloggin_dir = f\"/home/ubuntu/ziv/detrex/swarm_poc/autobrains_poc/tf_log/{job_id}\"\nswarm_one_client.download_tensorboard_logs(job_id, log_dir=loggin_dir, show_tensorboard=False)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "loggin_dir",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "peekOfCode": "loggin_dir = f\"/home/ubuntu/ziv/detrex/swarm_poc/autobrains_poc/tf_log/{job_id}\"\nswarm_one_client.download_tensorboard_logs(job_id, log_dir=loggin_dir, show_tensorboard=False)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.run_deta",
        "documentation": {}
    },
    {
        "label": "DETAModel",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "class DETAModel(L.LightningModule):\n    def __init__(self, model, criterion, postprocessors, args):\n        super().__init__()\n        self.model = model\n        self.criterion = criterion\n        self.postprocessors = postprocessors\n        self.args = args\n    def configure_optimizers(self):\n        param_dicts = [\n            {",
        "detail": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "def get_args_parser():\n    parser = argparse.ArgumentParser('Deformable DETR Detector', add_help=False)\n    parser.add_argument('--lr', default=5e-5, type=float)\n    parser.add_argument('--lr_backbone_names', default=[\"backbone.0\"], type=str, nargs='+')\n    parser.add_argument('--lr_backbone', default=5e-6, type=float)\n    parser.add_argument('--lr_linear_proj_names', default=['reference_points', 'sampling_offsets'], type=str, nargs='+')\n    parser.add_argument('--lr_linear_proj_mult', default=0.1, type=float)\n    parser.add_argument('--batch_size', default=2, type=int)\n    parser.add_argument('--weight_decay', default=1e-4, type=float)\n    parser.add_argument('--epochs', default=24, type=int)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "match_name_keywords",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "def match_name_keywords(n, name_keywords):\n    out = False\n    for b in name_keywords:\n        if b in n:\n            out = True\n            break\n    return out\nif args.dataset_file == \"coco_panoptic\":\n    # We also evaluate AP during panoptic training, on original coco DS\n    coco_val = datasets.coco.build(\"val\", args)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "def main(args):\n    model = DETAModel(model=model, criterion=criterion, postprocessors=postprocessors, args=args)\n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_loss',  \n        dirpath='./model_checkpoints/',\n        filename='deformable-detr-{epoch:02d}-{val_loss:.2f}',\n        save_top_k=3,  # Save the top 3 models according to the monitored metric\n        mode='min',  # 'min' mode saves models with the lowest 'monitor' metric\n    )\n    dataset_train = build_dataset(image_set='train', args=args)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "parser = argparse.ArgumentParser('Deformable DETR training and evaluation script', parents=[get_args_parser()])\ncustom_args = '--with_box_refine --two_stage \\\n    --num_feature_levels 5 --num_queries 900 \\\n    --dim_feedforward 2048 --dropout 0.0 --cls_loss_coef 1.0 \\\n    --assign_first_stage --assign_second_stage \\\n    --epochs 24 --lr_drop 20 \\\n    --lr 5e-5 --lr_backbone 5e-6 --batch_size 1 \\\n    --backbone swin \\\n    --bigger'.split()\nargs = parser.parse_args(custom_args)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "custom_args",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "custom_args = '--with_box_refine --two_stage \\\n    --num_feature_levels 5 --num_queries 900 \\\n    --dim_feedforward 2048 --dropout 0.0 --cls_loss_coef 1.0 \\\n    --assign_first_stage --assign_second_stage \\\n    --epochs 24 --lr_drop 20 \\\n    --lr 5e-5 --lr_backbone 5e-6 --batch_size 1 \\\n    --backbone swin \\\n    --bigger'.split()\nargs = parser.parse_args(custom_args)\nmodel, criterion, postprocessors = build_model(args)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "args = parser.parse_args(custom_args)\nmodel, criterion, postprocessors = build_model(args)\ndataset_train = build_dataset(image_set='train', args=args)\ndataset_val = build_dataset(image_set='val', args=args)\ndata_loader_train = DataLoader(dataset_train, 2, shuffle=True,\n                               collate_fn=utils.collate_fn, num_workers=0,\n                               pin_memory=True)\ndata_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "dataset_train",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "dataset_train = build_dataset(image_set='train', args=args)\ndataset_val = build_dataset(image_set='val', args=args)\ndata_loader_train = DataLoader(dataset_train, 2, shuffle=True,\n                               collate_fn=utils.collate_fn, num_workers=0,\n                               pin_memory=True)\ndata_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)\ndef match_name_keywords(n, name_keywords):\n    out = False",
        "detail": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "dataset_val",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "dataset_val = build_dataset(image_set='val', args=args)\ndata_loader_train = DataLoader(dataset_train, 2, shuffle=True,\n                               collate_fn=utils.collate_fn, num_workers=0,\n                               pin_memory=True)\ndata_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)\ndef match_name_keywords(n, name_keywords):\n    out = False\n    for b in name_keywords:",
        "detail": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "data_loader_train",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "data_loader_train = DataLoader(dataset_train, 2, shuffle=True,\n                               collate_fn=utils.collate_fn, num_workers=0,\n                               pin_memory=True)\ndata_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)\ndef match_name_keywords(n, name_keywords):\n    out = False\n    for b in name_keywords:\n        if b in n:",
        "detail": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "data_loader_val",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "data_loader_val = DataLoader(dataset_val, 2,\n                             drop_last=False, collate_fn=utils.collate_fn, num_workers=0,\n                             pin_memory=True)\ndef match_name_keywords(n, name_keywords):\n    out = False\n    for b in name_keywords:\n        if b in n:\n            out = True\n            break\n    return out",
        "detail": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "model = DETAModel(model=model,\n                  criterion=criterion,\n                  postprocessors=postprocessors,\n                  args=args)\nhyperparameters = {\n    \"max_epochs\": [5],\n    \"batch_sizes\": [16],\n}\ndef main(args):\n    model = DETAModel(model=model, criterion=criterion, postprocessors=postprocessors, args=args)",
        "detail": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "hyperparameters",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "description": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "peekOfCode": "hyperparameters = {\n    \"max_epochs\": [5],\n    \"batch_sizes\": [16],\n}\ndef main(args):\n    model = DETAModel(model=model, criterion=criterion, postprocessors=postprocessors, args=args)\n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_loss',  \n        dirpath='./model_checkpoints/',\n        filename='deformable-detr-{epoch:02d}-{val_loss:.2f}',",
        "detail": "swarm_poc.autobrains_poc.DETA-master.train_deta",
        "documentation": {}
    },
    {
        "label": "ContrastiveTransformations",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "class ContrastiveTransformations:\n    def __init__(self, base_transforms, n_views=2):\n        self.base_transforms = base_transforms\n        self.n_views = n_views\n    def __call__(self, x):\n        return [self.base_transforms(x) for i in range(self.n_views)]\ncontrast_transforms = transforms.Compose(\n    [\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomResizedCrop(size=224),",
        "detail": "swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "DinoV2Model",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "class DinoV2Model(L.LightningModule):\n    def __init__(self, learning_rate):\n        super().__init__()\n        # self.model = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vitg14\", pretrained=True)\n        self.model = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\", pretrained=True)\n        self.lr = learning_rate\n        self.temperature = 0.07\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=1e-4)\n        return optimizer",
        "detail": "swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "create_subset",
        "kind": 2,
        "importPath": "swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "def create_subset(dataset, subset_size=4096):\n    total_samples = len(dataset)\n    indices = np.random.choice(range(total_samples), subset_size, replace=True)\n    return Subset(dataset, indices)\nunlabeled_data_full = STL10(\n    root=DATASET_PATH,\n    split=\"unlabeled\",\n    download=False,\n    transform=ContrastiveTransformations(contrast_transforms, n_views=2),\n)",
        "detail": "swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "swarm_one_client",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "swarm_one_client = Client(api_key=\"API_KEY\")\n# ## Imports 📦\nimport lightning as L\nimport numpy as np\nimport os\nimport torch\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nfrom torchvision.datasets import STL10\nfrom torch.utils.data import DataLoader, Subset",
        "detail": "swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "DATASET_PATH",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"../stl10_data/\")\n# ## Contrastive Transformations\nclass ContrastiveTransformations:\n    def __init__(self, base_transforms, n_views=2):\n        self.base_transforms = base_transforms\n        self.n_views = n_views\n    def __call__(self, x):\n        return [self.base_transforms(x) for i in range(self.n_views)]\ncontrast_transforms = transforms.Compose(\n    [",
        "detail": "swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "contrast_transforms",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "contrast_transforms = transforms.Compose(\n    [\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomResizedCrop(size=224),\n        transforms.RandomApply([transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)], p=0.8),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.GaussianBlur(kernel_size=9),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,)),\n    ]",
        "detail": "swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "unlabeled_data_full",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "unlabeled_data_full = STL10(\n    root=DATASET_PATH,\n    split=\"unlabeled\",\n    download=False,\n    transform=ContrastiveTransformations(contrast_transforms, n_views=2),\n)\ntrain_data_contrast_full = STL10(\n    root=DATASET_PATH,\n    split=\"train\",\n    download=False,",
        "detail": "swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "train_data_contrast_full",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "train_data_contrast_full = STL10(\n    root=DATASET_PATH,\n    split=\"train\",\n    download=False,\n    transform=ContrastiveTransformations(contrast_transforms, n_views=2),\n)\nunlabeled_data = create_subset(unlabeled_data_full, subset_size=9000)\ntrain_data_contrast = create_subset(train_data_contrast_full, subset_size=1000)\n# ## DataLoaders\ntrain_dataloader = DataLoader(",
        "detail": "swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "unlabeled_data",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "unlabeled_data = create_subset(unlabeled_data_full, subset_size=9000)\ntrain_data_contrast = create_subset(train_data_contrast_full, subset_size=1000)\n# ## DataLoaders\ntrain_dataloader = DataLoader(\n    unlabeled_data,\n    batch_size=64,\n    shuffle=True,\n    drop_last=True,\n    pin_memory=True\n)",
        "detail": "swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "train_data_contrast",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "train_data_contrast = create_subset(train_data_contrast_full, subset_size=1000)\n# ## DataLoaders\ntrain_dataloader = DataLoader(\n    unlabeled_data,\n    batch_size=64,\n    shuffle=True,\n    drop_last=True,\n    pin_memory=True\n)\nval_dataloader = DataLoader(",
        "detail": "swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "train_dataloader",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "train_dataloader = DataLoader(\n    unlabeled_data,\n    batch_size=64,\n    shuffle=True,\n    drop_last=True,\n    pin_memory=True\n)\nval_dataloader = DataLoader(\n    train_data_contrast,\n    batch_size=64,",
        "detail": "swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "val_dataloader",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "val_dataloader = DataLoader(\n    train_data_contrast,\n    batch_size=64,\n    shuffle=False,\n    drop_last=False,\n    pin_memory=True\n)\n# ## DinoV2 LightningModule\nclass DinoV2Model(L.LightningModule):\n    def __init__(self, learning_rate):",
        "detail": "swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "model = DinoV2Model(learning_rate=0.01)\n# ## Training ⚡⚡⚡\n# You can abort any **JOB** / **TASK** at anytime to reduce your costs & total training time - from **your dashboard**\njob_id = swarm_one_client.fit(\n    model=model,\n    train_dataloaders=train_dataloader,\n    val_dataloaders=val_dataloader,\n    hyperparameters={\n        \"max_epochs\": [10],\n        \"batch_sizes\": [2048],",
        "detail": "swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "job_id",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "job_id = swarm_one_client.fit(\n    model=model,\n    train_dataloaders=train_dataloader,\n    val_dataloaders=val_dataloader,\n    hyperparameters={\n        \"max_epochs\": [10],\n        \"batch_sizes\": [2048],\n    },\n    name=f'AutoBrains Dino V2 Giant'\n)",
        "detail": "swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "trained_model",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "trained_model = swarm_one_client.download_trained_model(\"TASK_ID\", \".\")\nprint(trained_model)\nhistory = swarm_one_client.get_task_history(\"TASK_ID\")\nprint(history)\njob_history = swarm_one_client.get_job_history(job_id=job_id, to_pandas=True)\nprint(job_history)\njob_info = swarm_one_client.get_job_information(job_id, to_pandas=True)\nprint(job_info)",
        "detail": "swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "history",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "history = swarm_one_client.get_task_history(\"TASK_ID\")\nprint(history)\njob_history = swarm_one_client.get_job_history(job_id=job_id, to_pandas=True)\nprint(job_history)\njob_info = swarm_one_client.get_job_information(job_id, to_pandas=True)\nprint(job_info)",
        "detail": "swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "job_history",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "job_history = swarm_one_client.get_job_history(job_id=job_id, to_pandas=True)\nprint(job_history)\njob_info = swarm_one_client.get_job_information(job_id, to_pandas=True)\nprint(job_info)",
        "detail": "swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "job_info",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.dino_v2",
        "description": "swarm_poc.autobrains_poc.dino_v2",
        "peekOfCode": "job_info = swarm_one_client.get_job_information(job_id, to_pandas=True)\nprint(job_info)",
        "detail": "swarm_poc.autobrains_poc.dino_v2",
        "documentation": {}
    },
    {
        "label": "CustomDataset",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "class CustomDataset(Dataset):\n    def __init__(self, post_shape, split, num_samples=100):\n        self.post_shape = post_shape[1:]\n        self.num_samples = num_samples\n        self.cifar10 = datasets.CIFAR10(root=\"../data\", train=(split == \"train\"), download=False)\n        if split == \"train\":\n            transform_list = []\n            transform_list.extend([\n                v2.RandomHorizontalFlip(),\n                v2.RandomVerticalFlip(),",
        "detail": "swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "MobileNetV2",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "class MobileNetV2(L.LightningModule):\n    def __init__(self, num_classes, optimizer_type, learning_rate):\n        super().__init__()\n        self.model = mobilenet_v2(pretrained=True)\n        self.model.classifier = torch.nn.Sequential(torch.nn.Linear(self.model.last_channel, num_classes))\n        self.criterion = nn.CrossEntropyLoss()\n        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n        self.learning_rate = learning_rate\n        self.optimizer_type = optimizer_type\n    def forward(self, x):",
        "detail": "swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "swarm_one_client",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "swarm_one_client = Client(api_key=\"API_KEY\")\n# ## Imports 📦\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchmetrics\nimport lightning as L\nimport numpy as np\nfrom torchvision.models import mobilenet_v2\nfrom torchvision.transforms import Compose",
        "detail": "swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "train_dataset = CustomDataset(post_shape=(3, 224, 224),\n                              split=\"train\",\n                              num_samples=270_000)\nval_dataset = CustomDataset(post_shape=(3, 224, 224),\n                            split=\"val\",\n                            num_samples=30_000)\n# ## DataLoaders\ntrain_dataloader = DataLoader(train_dataset,\n                              batch_size=64,\n                              shuffle=True)",
        "detail": "swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "val_dataset = CustomDataset(post_shape=(3, 224, 224),\n                            split=\"val\",\n                            num_samples=30_000)\n# ## DataLoaders\ntrain_dataloader = DataLoader(train_dataset,\n                              batch_size=64,\n                              shuffle=True)\nval_dataloader = DataLoader(val_dataset,\n                            batch_size=64,\n                            shuffle=False)",
        "detail": "swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "train_dataloader",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "train_dataloader = DataLoader(train_dataset,\n                              batch_size=64,\n                              shuffle=True)\nval_dataloader = DataLoader(val_dataset,\n                            batch_size=64,\n                            shuffle=False)\nclass MobileNetV2(L.LightningModule):\n    def __init__(self, num_classes, optimizer_type, learning_rate):\n        super().__init__()\n        self.model = mobilenet_v2(pretrained=True)",
        "detail": "swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "val_dataloader",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "val_dataloader = DataLoader(val_dataset,\n                            batch_size=64,\n                            shuffle=False)\nclass MobileNetV2(L.LightningModule):\n    def __init__(self, num_classes, optimizer_type, learning_rate):\n        super().__init__()\n        self.model = mobilenet_v2(pretrained=True)\n        self.model.classifier = torch.nn.Sequential(torch.nn.Linear(self.model.last_channel, num_classes))\n        self.criterion = nn.CrossEntropyLoss()\n        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)",
        "detail": "swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "models",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "models = {\n    \"adam_lr_0.01\": MobileNetV2(num_classes=10, optimizer_type=\"adam\", learning_rate=0.01),\n    \"adam_lr_005\": MobileNetV2(num_classes=10, optimizer_type=\"adam\", learning_rate=0.005),\n    \"adamw_lr_005\": MobileNetV2(num_classes=10, optimizer_type=\"adamW\", learning_rate=0.005),\n}\nhyperparameters = {\n    \"max_epochs\": [10, 20],\n    \"batch_sizes\": [128, 256],\n}\n# ## Training ⚡⚡⚡",
        "detail": "swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "hyperparameters",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "hyperparameters = {\n    \"max_epochs\": [10, 20],\n    \"batch_sizes\": [128, 256],\n}\n# ## Training ⚡⚡⚡\n# You can abort any **JOB** / **TASK** at anytime to reduce your costs & total training time - from **your dashboard**\njob_id = swarm_one_client.fit(\n    model=models,\n    train_dataloaders=train_dataloader,\n    val_dataloaders=val_dataloader,",
        "detail": "swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "job_id",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "job_id = swarm_one_client.fit(\n    model=models,\n    train_dataloaders=train_dataloader,\n    val_dataloaders=val_dataloader,\n    hyperparameters=hyperparameters,\n    name=f'AutoBrains MobileNetV2',\n)\n# ## Tensorboard Logs 📉 📈\nswarm_one_client.download_tensorboard_logs(job_id, log_dir=\"path\", show_tensorboard=False)\n# ## Trained Model Downloading 🎯 🏆",
        "detail": "swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "trained_model",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "trained_model = swarm_one_client.download_trained_model(\"TASK_ID\", \".\")\nprint(trained_model)\nhistory = swarm_one_client.get_task_history(\"TASK_ID\")\nprint(history)\njob_history = swarm_one_client.get_job_history(job_id=job_id, to_pandas=True)\nprint(job_history)\njob_info = swarm_one_client.get_job_information(job_id, to_pandas=True)\nprint(job_info)",
        "detail": "swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "history",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "history = swarm_one_client.get_task_history(\"TASK_ID\")\nprint(history)\njob_history = swarm_one_client.get_job_history(job_id=job_id, to_pandas=True)\nprint(job_history)\njob_info = swarm_one_client.get_job_information(job_id, to_pandas=True)\nprint(job_info)",
        "detail": "swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "job_history",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "job_history = swarm_one_client.get_job_history(job_id=job_id, to_pandas=True)\nprint(job_history)\njob_info = swarm_one_client.get_job_information(job_id, to_pandas=True)\nprint(job_info)",
        "detail": "swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "job_info",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.mobilenet",
        "description": "swarm_poc.autobrains_poc.mobilenet",
        "peekOfCode": "job_info = swarm_one_client.get_job_information(job_id, to_pandas=True)\nprint(job_info)",
        "detail": "swarm_poc.autobrains_poc.mobilenet",
        "documentation": {}
    },
    {
        "label": "SimpleNN",
        "kind": 6,
        "importPath": "swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "class SimpleNN(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(28 * 28, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 10)\n    def forward(self, x):\n        x = self.flatten(x)\n        x = torch.relu(self.fc1(x))",
        "detail": "swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "swarm_one_client",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "swarm_one_client = Client(api_key=\"bSNxj8I21w\")\nimport lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n# Define a PyTorch Lightning module\nclass SimpleNN(pl.LightningModule):\n    def __init__(self):\n        super().__init__()",
        "detail": "swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "transform",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrain_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\nval_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n# Create data loaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [5, 10, 15],",
        "detail": "swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\nval_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n# Create data loaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [5, 10, 15],\n    \"batch_sizes\": [64]",
        "detail": "swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "val_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n# Create data loaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [5, 10, 15],\n    \"batch_sizes\": [64]\n}",
        "detail": "swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "train_loader",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [5, 10, 15],\n    \"batch_sizes\": [64]\n}\n# Train the model\njob_id = swarm_one_client.fit(",
        "detail": "swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "val_loader",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [5, 10, 15],\n    \"batch_sizes\": [64]\n}\n# Train the model\njob_id = swarm_one_client.fit(\n    model=model,",
        "detail": "swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "model = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [5, 10, 15],\n    \"batch_sizes\": [64]\n}\n# Train the model\njob_id = swarm_one_client.fit(\n    model=model,\n    train_dataloaders=train_loader,\n    val_dataloaders=val_loader,",
        "detail": "swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "hyperparameters",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "hyperparameters = {\n    \"max_epochs\": [5, 10, 15],\n    \"batch_sizes\": [64]\n}\n# Train the model\njob_id = swarm_one_client.fit(\n    model=model,\n    train_dataloaders=train_loader,\n    val_dataloaders=val_loader,\n    hyperparameters=hyperparameters,",
        "detail": "swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "job_id",
        "kind": 5,
        "importPath": "swarm_poc.autobrains_poc.pytorch_test",
        "description": "swarm_poc.autobrains_poc.pytorch_test",
        "peekOfCode": "job_id = swarm_one_client.fit(\n    model=model,\n    train_dataloaders=train_loader,\n    val_dataloaders=val_loader,\n    hyperparameters=hyperparameters,\n    name=\"MNIST test\"\n)\nswarm_one_client.download_tensorboard_logs(job_id, log_dir=\"path\", show_tensorboard=False)",
        "detail": "swarm_poc.autobrains_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "VERSION",
        "kind": 5,
        "importPath": "swarm_poc.swarm_one-0.0.4.setup",
        "description": "swarm_poc.swarm_one-0.0.4.setup",
        "peekOfCode": "VERSION = '0.0.4'\nDESCRIPTION = ' '\n_CURR_DIRECTORY = os.path.abspath(os.path.dirname(__file__))\nwith open(os.path.join(_CURR_DIRECTORY, \"requirements.txt\")) as f:\n   setup(\n      name=\"swarm_one\",\n      version=VERSION,\n      author=\"Kim Boren\",\n      author_email=\"kimb@r-stealth.com\",\n      description=DESCRIPTION,",
        "detail": "swarm_poc.swarm_one-0.0.4.setup",
        "documentation": {}
    },
    {
        "label": "DESCRIPTION",
        "kind": 5,
        "importPath": "swarm_poc.swarm_one-0.0.4.setup",
        "description": "swarm_poc.swarm_one-0.0.4.setup",
        "peekOfCode": "DESCRIPTION = ' '\n_CURR_DIRECTORY = os.path.abspath(os.path.dirname(__file__))\nwith open(os.path.join(_CURR_DIRECTORY, \"requirements.txt\")) as f:\n   setup(\n      name=\"swarm_one\",\n      version=VERSION,\n      author=\"Kim Boren\",\n      author_email=\"kimb@r-stealth.com\",\n      description=DESCRIPTION,\n      packages=find_packages(),",
        "detail": "swarm_poc.swarm_one-0.0.4.setup",
        "documentation": {}
    },
    {
        "label": "_CURR_DIRECTORY",
        "kind": 5,
        "importPath": "swarm_poc.swarm_one-0.0.4.setup",
        "description": "swarm_poc.swarm_one-0.0.4.setup",
        "peekOfCode": "_CURR_DIRECTORY = os.path.abspath(os.path.dirname(__file__))\nwith open(os.path.join(_CURR_DIRECTORY, \"requirements.txt\")) as f:\n   setup(\n      name=\"swarm_one\",\n      version=VERSION,\n      author=\"Kim Boren\",\n      author_email=\"kimb@r-stealth.com\",\n      description=DESCRIPTION,\n      packages=find_packages(),\n      install_requires=f.read().splitlines(),",
        "detail": "swarm_poc.swarm_one-0.0.4.setup",
        "documentation": {}
    },
    {
        "label": "init_wandb_run",
        "kind": 2,
        "importPath": "swarm_poc.logging_swarmone_wandb",
        "description": "swarm_poc.logging_swarmone_wandb",
        "peekOfCode": "def init_wandb_run(task_id, project_name):\n    return wandb.init(project=project_name, name=task_id, id=task_id, resume=\"allow\")\ndef log_metrics(task_id, metrics, epoch=None):\n    global last_logged\n    # Ensure task_id is initialized in last_logged\n    if task_id not in last_logged:\n        last_logged[task_id] = {}\n    for metric, values in metrics.items():\n        # Dynamically add new metrics to last_logged if they don't exist\n        if metric not in last_logged[task_id]:",
        "detail": "swarm_poc.logging_swarmone_wandb",
        "documentation": {}
    },
    {
        "label": "log_metrics",
        "kind": 2,
        "importPath": "swarm_poc.logging_swarmone_wandb",
        "description": "swarm_poc.logging_swarmone_wandb",
        "peekOfCode": "def log_metrics(task_id, metrics, epoch=None):\n    global last_logged\n    # Ensure task_id is initialized in last_logged\n    if task_id not in last_logged:\n        last_logged[task_id] = {}\n    for metric, values in metrics.items():\n        # Dynamically add new metrics to last_logged if they don't exist\n        if metric not in last_logged[task_id]:\n            last_logged[task_id][metric] = 0  # Initialize with 0, indicating no values logged yet\n        for i, value in enumerate(values):",
        "detail": "swarm_poc.logging_swarmone_wandb",
        "documentation": {}
    },
    {
        "label": "log_job_metrics",
        "kind": 2,
        "importPath": "swarm_poc.logging_swarmone_wandb",
        "description": "swarm_poc.logging_swarmone_wandb",
        "peekOfCode": "def log_job_metrics(job_id, project_name):\n    \"\"\"\n    Main process to fetch, log, and manage metrics for all tasks within a job.\n    \"\"\"\n    while True:\n        job_info = swarm_one_client.get_job_information(job_id)\n        all_completed = all(info['status'] in ['COMPLETED', 'ABORTED'] for info in job_info.values())\n        for task_id in job_info.keys():\n            wandb_run = init_wandb_run(task_id, project_name)\n            task_metrics = swarm_one_client.get_job_history(job_id, current=True).get(task_id, {})",
        "detail": "swarm_poc.logging_swarmone_wandb",
        "documentation": {}
    },
    {
        "label": "swarm_one_client",
        "kind": 5,
        "importPath": "swarm_poc.logging_swarmone_wandb",
        "description": "swarm_poc.logging_swarmone_wandb",
        "peekOfCode": "swarm_one_client = Client(api_key=\"API_KEY\")\nimport os\nimport wandb\nimport time\nos.environ[\"WANDB_DISABLE_SERVICE\"] = \"True\"\nos.environ[\"WANDB_API_KEY\"] = \"YOUR_WANDB_API_KEY\"\nlast_logged = {}\ndef init_wandb_run(task_id, project_name):\n    return wandb.init(project=project_name, name=task_id, id=task_id, resume=\"allow\")\ndef log_metrics(task_id, metrics, epoch=None):",
        "detail": "swarm_poc.logging_swarmone_wandb",
        "documentation": {}
    },
    {
        "label": "os.environ[\"WANDB_DISABLE_SERVICE\"]",
        "kind": 5,
        "importPath": "swarm_poc.logging_swarmone_wandb",
        "description": "swarm_poc.logging_swarmone_wandb",
        "peekOfCode": "os.environ[\"WANDB_DISABLE_SERVICE\"] = \"True\"\nos.environ[\"WANDB_API_KEY\"] = \"YOUR_WANDB_API_KEY\"\nlast_logged = {}\ndef init_wandb_run(task_id, project_name):\n    return wandb.init(project=project_name, name=task_id, id=task_id, resume=\"allow\")\ndef log_metrics(task_id, metrics, epoch=None):\n    global last_logged\n    # Ensure task_id is initialized in last_logged\n    if task_id not in last_logged:\n        last_logged[task_id] = {}",
        "detail": "swarm_poc.logging_swarmone_wandb",
        "documentation": {}
    },
    {
        "label": "os.environ[\"WANDB_API_KEY\"]",
        "kind": 5,
        "importPath": "swarm_poc.logging_swarmone_wandb",
        "description": "swarm_poc.logging_swarmone_wandb",
        "peekOfCode": "os.environ[\"WANDB_API_KEY\"] = \"YOUR_WANDB_API_KEY\"\nlast_logged = {}\ndef init_wandb_run(task_id, project_name):\n    return wandb.init(project=project_name, name=task_id, id=task_id, resume=\"allow\")\ndef log_metrics(task_id, metrics, epoch=None):\n    global last_logged\n    # Ensure task_id is initialized in last_logged\n    if task_id not in last_logged:\n        last_logged[task_id] = {}\n    for metric, values in metrics.items():",
        "detail": "swarm_poc.logging_swarmone_wandb",
        "documentation": {}
    },
    {
        "label": "last_logged",
        "kind": 5,
        "importPath": "swarm_poc.logging_swarmone_wandb",
        "description": "swarm_poc.logging_swarmone_wandb",
        "peekOfCode": "last_logged = {}\ndef init_wandb_run(task_id, project_name):\n    return wandb.init(project=project_name, name=task_id, id=task_id, resume=\"allow\")\ndef log_metrics(task_id, metrics, epoch=None):\n    global last_logged\n    # Ensure task_id is initialized in last_logged\n    if task_id not in last_logged:\n        last_logged[task_id] = {}\n    for metric, values in metrics.items():\n        # Dynamically add new metrics to last_logged if they don't exist",
        "detail": "swarm_poc.logging_swarmone_wandb",
        "documentation": {}
    },
    {
        "label": "SimpleNN",
        "kind": 6,
        "importPath": "swarm_poc.pytorch_test",
        "description": "swarm_poc.pytorch_test",
        "peekOfCode": "class SimpleNN(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(28*28, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 10)\n    def forward(self, x):\n        x = self.flatten(x)\n        x = torch.relu(self.fc1(x))",
        "detail": "swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "swarm_one_client",
        "kind": 5,
        "importPath": "swarm_poc.pytorch_test",
        "description": "swarm_poc.pytorch_test",
        "peekOfCode": "swarm_one_client = Client(api_key=\"API_KEY\")\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport lightning as pl\n# Define a PyTorch Lightning module\nclass SimpleNN(pl.LightningModule):\n    def __init__(self):\n        super().__init__()",
        "detail": "swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "transform",
        "kind": 5,
        "importPath": "swarm_poc.pytorch_test",
        "description": "swarm_poc.pytorch_test",
        "peekOfCode": "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrain_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\nval_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n# Create data loaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [10, 50, 100],",
        "detail": "swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "swarm_poc.pytorch_test",
        "description": "swarm_poc.pytorch_test",
        "peekOfCode": "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\nval_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n# Create data loaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [10, 50, 100],\n    \"batch_sizes\": [64]",
        "detail": "swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "swarm_poc.pytorch_test",
        "description": "swarm_poc.pytorch_test",
        "peekOfCode": "val_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n# Create data loaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [10, 50, 100],\n    \"batch_sizes\": [64]\n}",
        "detail": "swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "train_loader",
        "kind": 5,
        "importPath": "swarm_poc.pytorch_test",
        "description": "swarm_poc.pytorch_test",
        "peekOfCode": "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [10, 50, 100],\n    \"batch_sizes\": [64]\n}\n# Train the model\njob_id = swarm_one_client.fit(",
        "detail": "swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "val_loader",
        "kind": 5,
        "importPath": "swarm_poc.pytorch_test",
        "description": "swarm_poc.pytorch_test",
        "peekOfCode": "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n# Initialize and train the model using PyTorch Lightning\nmodel = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [10, 50, 100],\n    \"batch_sizes\": [64]\n}\n# Train the model\njob_id = swarm_one_client.fit(\n    model=model,",
        "detail": "swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "swarm_poc.pytorch_test",
        "description": "swarm_poc.pytorch_test",
        "peekOfCode": "model = SimpleNN()\nhyperparameters = {\n    \"max_epochs\": [10, 50, 100],\n    \"batch_sizes\": [64]\n}\n# Train the model\njob_id = swarm_one_client.fit(\n    model=model,\n    train_dataloaders=train_loader,\n    val_dataloaders=val_loader,",
        "detail": "swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "hyperparameters",
        "kind": 5,
        "importPath": "swarm_poc.pytorch_test",
        "description": "swarm_poc.pytorch_test",
        "peekOfCode": "hyperparameters = {\n    \"max_epochs\": [10, 50, 100],\n    \"batch_sizes\": [64]\n}\n# Train the model\njob_id = swarm_one_client.fit(\n    model=model,\n    train_dataloaders=train_loader,\n    val_dataloaders=val_loader,\n    hyperparameters=hyperparameters,",
        "detail": "swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "job_id",
        "kind": 5,
        "importPath": "swarm_poc.pytorch_test",
        "description": "swarm_poc.pytorch_test",
        "peekOfCode": "job_id = swarm_one_client.fit(\n    model=model,\n    train_dataloaders=train_loader,\n    val_dataloaders=val_loader,\n    hyperparameters=hyperparameters,\n    name=\"MNIST test\"\n)\nif __name__ == \"__main__\":\n    project_name = 'your_project_name'  # OR the job_id\n    from logging_swarmone_wandb import log_job_metrics",
        "detail": "swarm_poc.pytorch_test",
        "documentation": {}
    },
    {
        "label": "init_clearml_task",
        "kind": 2,
        "importPath": "logging_swarmone_clearml",
        "description": "logging_swarmone_clearml",
        "peekOfCode": "def init_clearml_task(swarm_task_id, project_name):\n    try:\n        clearml_task_id = clearml_tasks_task_ids[swarm_task_id]\n        clearml_task = Task.get_task(task_id=clearml_task_id)\n        return clearml_task\n    except KeyError as e:\n        clearml_task = Task.create(project_name=project_name, task_name=swarm_task_id,\n                                   task_type=Task.TaskTypes.training)\n        clearml_tasks_task_ids[swarm_task_id] = clearml_task.task_id\n    return clearml_task",
        "detail": "logging_swarmone_clearml",
        "documentation": {}
    },
    {
        "label": "log_metrics",
        "kind": 2,
        "importPath": "logging_swarmone_clearml",
        "description": "logging_swarmone_clearml",
        "peekOfCode": "def log_metrics(task_id, metrics, clearml_task):\n    global last_logged\n    if task_id not in last_logged:\n        last_logged[task_id] = {}\n    for metric, values in metrics.items():\n        # Dynamically add new metrics to last_logged if they don't exist\n        if metric not in last_logged[task_id]:\n            last_logged[task_id][metric] = 0  # Initialize with 0, indicating no values logged yet\n        for i, value in enumerate(values):\n            # Only log if this index hasn't been logged before",
        "detail": "logging_swarmone_clearml",
        "documentation": {}
    },
    {
        "label": "log_job_metrics",
        "kind": 2,
        "importPath": "logging_swarmone_clearml",
        "description": "logging_swarmone_clearml",
        "peekOfCode": "def log_job_metrics(job_id, project_name, swarm_one_client):\n    \"\"\"\n    Main process to fetch, log, and manage metrics for all tasks within a job.\n    \"\"\"\n    while True:\n        completed_or_aborted_task_ids = []\n        job_info = swarm_one_client.get_job_information(job_id)\n        all_completed = all(info['status'] in ['COMPLETED', 'ABORTED'] for info in job_info.values())\n        job_metrics = swarm_one_client.get_job_history(job_id, current=True)\n        for task_id, info in job_info.items():",
        "detail": "logging_swarmone_clearml",
        "documentation": {}
    },
    {
        "label": "last_logged",
        "kind": 5,
        "importPath": "logging_swarmone_clearml",
        "description": "logging_swarmone_clearml",
        "peekOfCode": "last_logged = {}\nclearml_tasks_task_ids = {}\ndef init_clearml_task(swarm_task_id, project_name):\n    try:\n        clearml_task_id = clearml_tasks_task_ids[swarm_task_id]\n        clearml_task = Task.get_task(task_id=clearml_task_id)\n        return clearml_task\n    except KeyError as e:\n        clearml_task = Task.create(project_name=project_name, task_name=swarm_task_id,\n                                   task_type=Task.TaskTypes.training)",
        "detail": "logging_swarmone_clearml",
        "documentation": {}
    },
    {
        "label": "clearml_tasks_task_ids",
        "kind": 5,
        "importPath": "logging_swarmone_clearml",
        "description": "logging_swarmone_clearml",
        "peekOfCode": "clearml_tasks_task_ids = {}\ndef init_clearml_task(swarm_task_id, project_name):\n    try:\n        clearml_task_id = clearml_tasks_task_ids[swarm_task_id]\n        clearml_task = Task.get_task(task_id=clearml_task_id)\n        return clearml_task\n    except KeyError as e:\n        clearml_task = Task.create(project_name=project_name, task_name=swarm_task_id,\n                                   task_type=Task.TaskTypes.training)\n        clearml_tasks_task_ids[swarm_task_id] = clearml_task.task_id",
        "detail": "logging_swarmone_clearml",
        "documentation": {}
    }
]